{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18068f-9693-4e37-9183-3be895fee7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cluster import MeanShift, DBSCAN\n",
    "from sklearn.metrics import roc_auc_score, r2_score, mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit, ols, glm\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import chi2_contingency, chisquare, ttest_1samp\n",
    "import umap\n",
    "import numba\n",
    "import sklearn\n",
    "import shap\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23513396-0786-403b-b71a-b5991343e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(umap.__version__)\n",
    "print(np.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(numba.__version__)\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f114e1-0722-4244-8b57-7bf40cb88ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell_do(command, log=False, return_log=False):\n",
    "    print(f'Executing: {(\" \").join(command.split())}', file=sys.stderr)\n",
    "\n",
    "    res=subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    if log:\n",
    "        print(res.stdout.decode('utf-8'))\n",
    "    if return_log:\n",
    "        return(res.stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b475e44-62c4-43e4-adb4-45e55dad59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(labeled_df, color, fname, symbol=None, x='PC1', y='PC2', z='PC3', title=None, x_range=None, y_range=None, z_range=None):\n",
    "    '''\n",
    "    Parameters: \n",
    "    labeled_df (Pandas dataframe): labeled ancestry dataframe\n",
    "    color (string): color of ancestry label. column name containing labels for ancestry in labeled_pcs_df\n",
    "    symbol (string): symbol of secondary label (for example, predicted vs reference ancestry). default: None\n",
    "    plot_out (string): filename to output filename for .png and .html plotly images\n",
    "    x (string): column name of x-dimension\n",
    "    y (string): column name of y-dimension\n",
    "    z (string): column name of z-dimension\n",
    "    title (string, optional): title of output scatterplot\n",
    "    x_range (list of floats [min, max], optional): range for x-axis\n",
    "    y_range (list of floats [min, max], optional): range for y-axis\n",
    "    z_range (list of floats [min, max], optional): range for z-axis\n",
    "\n",
    "    Returns:\n",
    "    3-D scatterplot (plotly.express.scatter_3d). If plot_out included, will write .png static image and .html interactive to plot_out filename\n",
    "        \n",
    "    '''\n",
    "    fig = px.scatter_3d(\n",
    "        labeled_df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        color=color,\n",
    "        symbol=symbol,\n",
    "        title=title,\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "        range_x=x_range,\n",
    "        range_y=y_range,\n",
    "        range_z=z_range\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(marker={'size': 3})\n",
    "    \n",
    "    fig.show()\n",
    "    fig.write_html(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295d85e-c83e-4562-9398-5bcfe1e95d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(transform, labels, name, n_comps, fname):\n",
    "    # take transform np array and make if a pd dataframe\n",
    "    transform_df = pd.DataFrame(transform)\n",
    "    col_names = ['COMP'+str(i+1) for i in range(n_comps)]\n",
    "    transform_df.columns = col_names\n",
    "    \n",
    "    # add labels column\n",
    "    transform_df.loc[:, 'label'] = labels\n",
    "    \n",
    "    # plot in 2d (don't think is is needed anymore)\n",
    "    if n_comps < 3:\n",
    "        sns.lmplot(x='COMP1', y='COMP2', hue='label', data=transform_df, fit_reg=False)\n",
    "        plt.show()\n",
    "\n",
    "    # plot in 3d\n",
    "    else:\n",
    "        plot_3d(labeled_df=transform_df, color='label', fname=fname, title=name, x='COMP1', y='COMP2', z='COMP3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec3fc4-23a2-45d1-b7fd-2fe8db419859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(data, title, x_lab, y_lab):\n",
    "    # plot heatmap for Disease n ~ Comp/Cluster X regressions\n",
    "    sns.heatmap(data, annot=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_lab)\n",
    "    plt.ylabel(y_lab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff835f72-fcd5-44c2-a6fb-104922eb67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up(cases, disease=None):\n",
    "    # isolate a single disease if requested\n",
    "    if disease:\n",
    "        cases = cases[cases['PHENO'] == disease]\n",
    "        \n",
    "    # get X data\n",
    "    X = cases.drop(columns=['PHENO'], axis=1)\n",
    "    print(X.shape)\n",
    "    \n",
    "    # get y data\n",
    "    y = cases['PHENO']\n",
    "    print(y.shape)\n",
    "    print(y.value_counts())\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260559b6-dede-455e-9c83-c104b201b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_transform(X_train, X_test, y_cases, a, b, seed=None, cohorts=None, plot_transform=True):\n",
    "    wd = 'insert_path'\n",
    "    \n",
    "    # umap transform train and test data\n",
    "    umap = UMAP(n_components=3, a=a, b=b, random_state=seed)\n",
    "    X_train_umap = umap.fit_transform(X_train)\n",
    "    X_test_umap = umap.transform(X_test)\n",
    "    \n",
    "    # get full umap data\n",
    "    X_cases_umap = np.append(X_train_umap, X_test_umap, axis=0)\n",
    "    \n",
    "    # plot when requested\n",
    "    if plot_transform:\n",
    "        plot(X_cases_umap, y_cases, 'UMAP', X_cases_umap.shape[1], f'{wd}/analysis/figures/umap.html')\n",
    "        if cohorts is not None:\n",
    "            plot(X_cases_umap, cohorts, 'UMAP', X_cases_umap.shape[1], f'{wd}/analysis/figures/umap_cohorts.html')\n",
    "    \n",
    "    return X_train_umap, X_test_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac92d4-2d1d-451f-a48e-4e89634a3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permuted_clusters(cluster_membership_gen_path, y_ids, disease):\n",
    "    # num iterations, set up membership dict\n",
    "    iterations = 100\n",
    "    all_c0_ids_dict = {}\n",
    "    not_c0_ids_dict = {}\n",
    "    \n",
    "    # loop through train/test split iterations\n",
    "    for i in range(15):\n",
    "        # get correct path and read\n",
    "        cluster_membership_path = cluster_membership_gen_path.replace('*',str(i+1))\n",
    "        cluster_membership_df = pd.read_csv(cluster_membership_path, sep='\\s+')\n",
    "        \n",
    "        # drop id, pheno, get average cluster membership over 100 iterations\n",
    "        cluster_membership = cluster_membership_df.drop(columns=['ID','pheno'], axis=1)\n",
    "        cluster_membership = cluster_membership.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "        cluster_membership_avg = cluster_membership/iterations\n",
    "\n",
    "        # copy id and pheno to new dfs\n",
    "        cluster_membership['ID'] = cluster_membership_df['ID'].copy()\n",
    "        cluster_membership_avg['ID'] = cluster_membership_df['ID'].copy()\n",
    "        cluster_membership['pheno'] = cluster_membership_df['pheno'].copy()\n",
    "        cluster_membership_avg['pheno'] = cluster_membership_df['pheno'].copy()\n",
    "    \n",
    "        # gettign ids for all c0 and not c0\n",
    "        all_c0 = cluster_membership_avg[cluster_membership_avg[0] == 1.0]\n",
    "        not_c0 = cluster_membership_avg[cluster_membership_avg[0] == 0.0]\n",
    "        all_c0_ids_dict[i] = list(all_c0['ID'])\n",
    "        not_c0_ids_dict[i] = list(not_c0['ID'])\n",
    "    \n",
    "    all_c0_overlap = []\n",
    "    not_c0_overlap = []\n",
    "    sometimes_c0_overlap = []\n",
    "\n",
    "    # loop through ids\n",
    "    for iid in y_ids:\n",
    "        sum_all_c0 = 0\n",
    "        sum_not_c0 = 0\n",
    "        \n",
    "        # see if id is in each iteration\n",
    "        for key in all_c0_ids_dict:\n",
    "            if iid in all_c0_ids_dict[key]:\n",
    "                sum_all_c0 += 1\n",
    "            if iid in not_c0_ids_dict[key]:\n",
    "                sum_not_c0 += 1\n",
    "        \n",
    "        if disease:\n",
    "            all_threshold = len(all_c0_ids_dict)-3\n",
    "            not_threshold = len(not_c0_ids_dict)-3\n",
    "        else:\n",
    "            all_threshold = len(all_c0_ids_dict)\n",
    "            not_threshold = len(not_c0_ids_dict)\n",
    "        \n",
    "        # get all c0 ids (c0)\n",
    "        if sum_all_c0 >= all_threshold:\n",
    "            all_c0_overlap.append(iid)\n",
    "        \n",
    "        # get not c0 ids (c2)\n",
    "        if sum_not_c0 >= not_threshold:\n",
    "            not_c0_overlap.append(iid)\n",
    "\n",
    "        # rest are sometimes c0 ids (c1)\n",
    "        if (iid not in all_c0_overlap) and (iid not in not_c0_overlap):\n",
    "            sometimes_c0_overlap.append(iid)\n",
    "\n",
    "    # assigning cluster memebership\n",
    "    all_c0 = pd.DataFrame()\n",
    "    not_c0 = pd.DataFrame()\n",
    "    sometimes_c0 = pd.DataFrame()\n",
    "    print(f'C0:{len(all_c0_overlap)}')\n",
    "    print(f'C1:{len(sometimes_c0_overlap)}')\n",
    "    print(f'C2:{len(not_c0_overlap)}')\n",
    "    \n",
    "    all_c0['ID'] = all_c0_overlap\n",
    "    not_c0['ID'] = not_c0_overlap\n",
    "    sometimes_c0['ID'] = sometimes_c0_overlap\n",
    "\n",
    "    all_c0['cluster'] = 0\n",
    "    not_c0['cluster'] = 2\n",
    "    sometimes_c0['cluster'] = 1\n",
    "    \n",
    "    # concatenate full data\n",
    "    full = pd.concat([all_c0,not_c0,sometimes_c0], axis=0, ignore_index=True)\n",
    "    print(full.head())\n",
    "    print(full.shape)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1866273-e0f2-4ab2-945e-589e45c3ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_cluster_data(cluster_data):\n",
    "    # removing cases that were not clustered (not an issue with MeanShift)\n",
    "    cluster_data = cluster_data[cluster_data['cluster'] != -1]\n",
    "    \n",
    "    # get dummies for pheno and cluster membership for regression\n",
    "    cluster_data = pd.concat([cluster_data, pd.get_dummies(cluster_data['pheno']), pd.get_dummies(cluster_data['cluster'], prefix='cluster')], axis=1)\n",
    "    rename_dict = {}\n",
    "\n",
    "    # get comp columns to rename\n",
    "    for column in cluster_data.columns:\n",
    "        if type(column) is int:\n",
    "            rename_dict[column] = 'COMP' + str(column+1)\n",
    "\n",
    "    # rename columns\n",
    "    cluster_data = cluster_data.rename(columns=rename_dict)\n",
    "    print(cluster_data.shape)\n",
    "    \n",
    "    return cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2906533-52a8-45eb-8731-b37699f3ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disease_regression(train_data, test_data, data_type, standardize=False, simul=False):\n",
    "    # copy of train and test data\n",
    "    train_data_copy = train_data.copy(deep=True)\n",
    "    test_data_copy = test_data.copy(deep=True)\n",
    "    \n",
    "    # disease list\n",
    "    diseases = ['ad','pd','als','lbd','ftd']\n",
    "    \n",
    "    # set up some data lists\n",
    "    coef_data = []\n",
    "    or_data =[]\n",
    "    z_data = []\n",
    "    p_data = []\n",
    "    se_data = []\n",
    "    \n",
    "    # get cluster cols\n",
    "    cluster_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'cluster_' in column:\n",
    "            cluster_cols.append(column)\n",
    "    \n",
    "    # get comp cols\n",
    "    comp_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'COMP' in column:\n",
    "            comp_cols.append(column)\n",
    "    \n",
    "    # set up cluster regression\n",
    "    if data_type == 'clusters':\n",
    "        cols = cluster_cols\n",
    "        axis = 'Cluster Membership'\n",
    "    # set up comp regression\n",
    "    else:\n",
    "        cols = comp_cols\n",
    "        # standardize when requested\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            train_data_copy[comp_cols] = scaler.fit_transform(train_data_copy[comp_cols])\n",
    "            test_data_copy[comp_cols] = scaler.fit_transform(test_data_copy[comp_cols])\n",
    "            axis = 'Standardized Component'\n",
    "        else:\n",
    "            axis = 'Component'\n",
    "    \n",
    "    auc_data = []\n",
    "    \n",
    "    full_data_copy = pd.concat([train_data_copy, test_data_copy], axis=0)\n",
    "    \n",
    "    # loop through diseases\n",
    "    for disease in diseases:\n",
    "        coef_row_data = []\n",
    "        or_row_data = []\n",
    "        z_row_data = []\n",
    "        p_row_data = []\n",
    "        se_row_data = []\n",
    "        # Disease n ~ Comp/Cluster 1 + ... + Comp/Cluster n regression\n",
    "        if simul:\n",
    "            formula_str = ''\n",
    "        \n",
    "            for i in range(len(cols)):\n",
    "                if data_type == 'clusters':\n",
    "                    if cols[i] != cols[-2]:\n",
    "                        if cols[i] != cols[-1]:\n",
    "                            formula_str += str(cols[i]) + ' + '\n",
    "                        else:\n",
    "                            formula_str += str(cols[i])\n",
    "                else:\n",
    "                    if cols[i] != cols[-1]:\n",
    "                            formula_str += str(cols[i]) + ' + '\n",
    "                    else:\n",
    "                        formula_str += str(cols[i])\n",
    "            \n",
    "            formula = (f'{disease} ~ {formula_str}')\n",
    "            model = logit(formula=formula, data=train_data_copy).fit(disp=0)\n",
    "            pred = model.predict(test_data_copy[cols])\n",
    "            # print(roc_auc_score(test_data_copy[disease], pred))\n",
    "            auc_data.append(roc_auc_score(test_data_copy[disease], pred))\n",
    "            results = pd.read_html(model.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
    "            coef_data.append(results['coef'])\n",
    "            or_data.append(np.exp(results['coef']))\n",
    "            z_data.append(results['z'])\n",
    "            p_data.append(results['P>|z|'])\n",
    "            \n",
    "        # Disease n ~ Comp/Cluster n regression\n",
    "        else:\n",
    "            for col in cols:\n",
    "                formula = (f'{disease} ~ {col}')\n",
    "                model = logit(formula=formula, data=train_data_copy).fit(disp=0)\n",
    "                pred = model.predict(test_data_copy[col])\n",
    "                # print(roc_auc_score(test_data_copy[disease], pred))\n",
    "                auc_data.append(roc_auc_score(test_data_copy[disease], pred))\n",
    "                results = pd.read_html(model.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
    "                coef_row_data.append(results.iloc[1]['coef'])\n",
    "                or_row_data.append(np.exp(results.iloc[1]['coef']))\n",
    "                z_row_data.append(results.iloc[1]['z'])\n",
    "                p_row_data.append(model.pvalues.loc[col])\n",
    "                se_row_data.append(results.iloc[1]['std err'])\n",
    "    \n",
    "            coef_data.append(coef_row_data)\n",
    "            or_data.append(or_row_data)\n",
    "            z_data.append(z_row_data)\n",
    "            p_data.append(p_row_data)\n",
    "            se_data.append(se_row_data)\n",
    "    \n",
    "    # dataframe heatmap data and plot\n",
    "    coef_data = pd.DataFrame(coef_data, index=diseases)\n",
    "    or_data = pd.DataFrame(or_data, index=diseases)\n",
    "    z_data = pd.DataFrame(z_data, index=diseases)\n",
    "    p_data = pd.DataFrame(p_data, index=diseases)\n",
    "    se_data = pd.DataFrame(se_data, index=diseases)\n",
    "    \n",
    "    print('OR')\n",
    "    print(or_data)\n",
    "    print('BETA')\n",
    "    print(coef_data)\n",
    "    print('SE')\n",
    "    print(se_data)\n",
    "    print('P')\n",
    "    print(p_data)\n",
    "    \n",
    "    # plot_heatmap(or_data, f'Disease Status vs. {axis} - OR', axis, 'Disease Status')\n",
    "    # plot_heatmap(coef_data, f'Disease Status vs. {axis} - Coefficients', axis, 'Disease Status')\n",
    "    # plot_heatmap(z_data, f'Disease Status vs. {axis} - Z scores', axis, 'Disease Status')\n",
    "    # plot_heatmap(se_data, f'Disease Status vs. {axis} - Std Err', axis, 'Disease Status')\n",
    "    # plot_heatmap(p_data, f'Disease Status vs. {axis} - P values', axis, 'Disease Status')\n",
    "    \n",
    "    # check out average AUC\n",
    "    print('Average AUC')\n",
    "    print(np.mean(auc_data))\n",
    "    \n",
    "    return p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73fc8ea-8000-461e-b862-cbea6e04ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_data(X_train, X_test, train_clusters, test_clusters):\n",
    "    # concat X_train with cluster data\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    train_full = pd.concat([X_train, train_clusters], axis=1)\n",
    "    \n",
    "    # concat X_test with cluster data\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    test_full = pd.concat([X_test, test_clusters], axis=1)\n",
    "    \n",
    "    return train_full, test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911c7a1-8745-4ee4-98c9-66f435bfea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_names(train_full, test_full, disease_snp_assoc_path, gene_list_path, geno_path):\n",
    "    # identify SNP and non-SNP columns\n",
    "    snp_cols = []\n",
    "    non_snp_cols = []\n",
    "    for column in train_full.columns:\n",
    "        if 'rs' in column:\n",
    "            snp_cols.append(column)\n",
    "        else:\n",
    "            non_snp_cols.append(column)\n",
    "        # for when test set does not have any cases that belong to cluster_x\n",
    "        if column not in test_full.columns:\n",
    "            test_full[column] = 0\n",
    "            test_full = test_full[train_full.columns]\n",
    "            \n",
    "    snp_cols = pd.DataFrame(snp_cols)\n",
    "    snp_cols.columns = ['SNP']\n",
    "\n",
    "    # read in disease snp assoc file\n",
    "    disease_snp_assoc = pd.read_csv(f'{disease_snp_assoc_path}', sep=',')\n",
    "    disease_snp_assoc.columns = ['D1','D2','SNP','GENE']\n",
    "    \n",
    "    # read in gene postions file\n",
    "    gene_list = pd.read_csv(gene_list_path, sep=',')\n",
    "    gene_list = gene_list[(gene_list['CHR'] != 'X') & (gene_list['CHR'] != 'Y') & (gene_list['CHR'] != 'XY')]\n",
    "    gene_list['CHR'] = gene_list['CHR'].astype(int)\n",
    "    \n",
    "    # read in bim file \n",
    "    bim = pd.read_csv(f'{geno_path}.bim', sep='\\s+', header=None)\n",
    "    bim.columns = ['chr','SNP','pos','bp','alt','ref']\n",
    "    \n",
    "    for snp in snp_cols.values:\n",
    "        if snp not in disease_snp_assoc['SNP'].values:\n",
    "            row = {'D1':'AD','D2':np.nan,'SNP':snp}\n",
    "            disease_snp_assoc = pd.concat([disease_snp_assoc,pd.DataFrame(row)], axis=0, ignore_index=True)\n",
    "\n",
    "    # merge disease snp assoc with snp cols\n",
    "    disease_snp_guide = snp_cols.merge(disease_snp_assoc, how='inner', on=['SNP'])\n",
    "    \n",
    "    # merge disease_snp_guide with bim\n",
    "    disease_snp_guide_merge = disease_snp_guide.merge(bim, how='inner', on=['SNP'])\n",
    "    \n",
    "    col_name_strs = []\n",
    "\n",
    "    # rename snps based on disease assoc\n",
    "    for index, row in disease_snp_guide_merge.iterrows():\n",
    "        col_name_str = ''\n",
    "        if pd.isna(row['D2']):\n",
    "            col_name_str = f'{row[\"SNP\"]}_{row[\"D1\"]}'\n",
    "        else:\n",
    "            col_name_str = f'{row[\"SNP\"]}_{row[\"D1\"]}_{row[\"D2\"]}'\n",
    "\n",
    "        if not pd.isna(row['GENE']):\n",
    "            row['GENE'] = row['GENE'].replace('-','_')\n",
    "            col_name_str += f'_{row[\"GENE\"]}'\n",
    "        else:\n",
    "            chrom = row['chr']\n",
    "            lower = row['bp'] - 400000\n",
    "            upper = row['bp'] + 400000\n",
    "            gene_list_chr = gene_list[gene_list['CHR'] == chrom]\n",
    "            gene_list_range = gene_list_chr[(gene_list_chr['START'] > lower) & (gene_list_chr['STOP'] < upper)]\n",
    "            gene_list_range_cp = gene_list_range.copy()\n",
    "            gene_list_range_cp['low_dist'] = gene_list_chr['START'] - lower\n",
    "            gene_list_range_cp['high_dist'] = upper - gene_list_chr['STOP']\n",
    "            gene_list_range_cp['min_dist'] = gene_list_range_cp[['low_dist','high_dist']].min(axis=1)\n",
    "            gene_list_range_cp = gene_list_range_cp[gene_list_range_cp['min_dist'] == gene_list_range_cp['min_dist'].min()]\n",
    "            for ind in gene_list_range_cp.index:\n",
    "                col_name_str += f'_{gene_list_range_cp[\"GENE\"][ind]}'\n",
    "            \n",
    "        col_name_strs.append(col_name_str)\n",
    "\n",
    "    disease_snp_guide_merge['col_name'] = col_name_strs\n",
    "    \n",
    "    # get and set full column names\n",
    "    new_col_names = disease_snp_guide_merge['col_name']\n",
    "    col_names_full = pd.concat([new_col_names, pd.Series(non_snp_cols)])\n",
    "    train_full.columns = col_names_full\n",
    "    test_full.columns = col_names_full\n",
    "    \n",
    "    return train_full, test_full, col_names_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865c504-4115-4b2f-a9fe-c358074cdb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_regression(train_data, test_data, data_type, standardize=False, plot_shap=True):\n",
    "    # copy of train and test data\n",
    "    train_data_copy = train_data.copy(deep=True)\n",
    "    test_data_copy = test_data.copy(deep=True)\n",
    "    \n",
    "    # get cluster cols\n",
    "    cluster_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'cluster_' in column:\n",
    "            cluster_cols.append(column)\n",
    "    \n",
    "    # get comp cols\n",
    "    comp_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'COMP' in column:\n",
    "            comp_cols.append(column)\n",
    "    \n",
    "    # get snp cols\n",
    "    snp_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'rs' in column:\n",
    "            snp_cols.append(column)\n",
    "    \n",
    "    # set up cluster regression\n",
    "    if data_type == 'clusters':\n",
    "        cols = cluster_cols\n",
    "        axis = 'Cluster Membership'\n",
    "    # set up comp regression\n",
    "    else:\n",
    "        cols = comp_cols\n",
    "        # standardize when requested\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            train_data_copy[comp_cols] = scaler.fit_transform(train_data_copy[comp_cols])\n",
    "            test_data_copy[comp_cols] = scaler.transform(test_data_copy[comp_cols])\n",
    "            axis = 'Standardized Component'\n",
    "        else:\n",
    "            axis = 'Component'\n",
    "    \n",
    "    top_snps = {}\n",
    "    \n",
    "    full_data_copy = pd.concat([train_data_copy, test_data_copy], axis=0)\n",
    "    \n",
    "    # loop through comps/clusters\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        # SNPs ~ Cluster n (log reg)\n",
    "        if data_type == 'clusters':\n",
    "            model = LogisticRegression(max_iter=200)\n",
    "            model.fit(train_data_copy[snp_cols], train_data_copy[col])\n",
    "            pred = model.predict_proba(test_data_copy[snp_cols])[:,1]\n",
    "            print(roc_auc_score(test_data_copy[col], pred))\n",
    "            explainer = shap.Explainer(model, train_data_copy[snp_cols])\n",
    "            shap_values = explainer(train_data_copy[snp_cols], max_evals=501)\n",
    "        # SNPs ~ Comp n (lin reg)\n",
    "        else:\n",
    "            model = LinearRegression()\n",
    "            model.fit(train_data_copy[snp_cols], train_data_copy[col])\n",
    "            pred = model.predict(test_data_copy[snp_cols])\n",
    "            print(mean_squared_error(test_data_copy[col], pred))\n",
    "            explainer = shap.Explainer(model, test_data_copy[snp_cols])\n",
    "            shap_values = explainer(train_data_copy[snp_cols], max_evals=501)\n",
    "        \n",
    "        # get shap importance values (descending)\n",
    "        feature_names = shap_values.feature_names\n",
    "        shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
    "        vals = np.abs(shap_df.values).mean(0)\n",
    "        shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
    "        shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
    "        top_snps[col] = list(shap_importance.head(20)['col_name'])\n",
    "        \n",
    "        # shap plots - need to make bar plot bidirectional\n",
    "        if plot_shap:\n",
    "            shap.summary_plot(shap_values, train_data_copy[snp_cols], show=False)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            shap.plots.bar(shap_values, max_display=20, show=False)\n",
    "            f = plt.gcf()\n",
    "            f.savefig(f'/data/CARD/projects/AD_Cluster/analysis/figures/shap_summary_{col}.png', bbox_inches='tight', dpi=400)\n",
    "            plt.show()\n",
    "\n",
    "    return top_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a9838-b122-42af-9ba5-06ee968e7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disease_specific_prs(adjusted, train_full, test_full, geno_path, assoc_path, out_path):\n",
    "    # disease list\n",
    "    diseases = ['ad','pd','als','lbd','ftd']\n",
    "    \n",
    "    # drop ID column, write columns to txt\n",
    "    adj = adjusted.drop(columns=['ID'], axis=1)\n",
    "    adj_cols = pd.Series(list(adj.columns))\n",
    "    adj_cols.to_csv(f'{out_path}_variant_ids.txt', sep='\\t', index=False, header=False)\n",
    "    \n",
    "    # extract variants from geno\n",
    "    extract_cmd = f'plink2 --bfile {geno_path} --extract {out_path}_variant_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(extract_cmd)\n",
    "    \n",
    "    # read assoc file\n",
    "    assoc = pd.read_csv(assoc_path, sep='\\s+', header=None)\n",
    "    assoc.columns = ['ID','ALLELE','BETA','DISEASE']\n",
    "    \n",
    "    train_id = train_full['ID']\n",
    "    test_id = test_full['ID']\n",
    "    train_out_path = f'{out_path}_train'\n",
    "    test_out_path = f'{out_path}_test'\n",
    "    pd.concat([train_id, train_id], axis=1).to_csv(f'{train_out_path}_ids.txt', sep='\\t', index=False, header=False)\n",
    "    pd.concat([test_id, test_id], axis=1).to_csv(f'{test_out_path}_ids.txt', sep='\\t', index=False, header=False)\n",
    "    \n",
    "    # isolate train and test plink data\n",
    "    keep_train_cmd = f'plink2 --bfile {out_path} --keep {train_out_path}_ids.txt --make-bed --out {train_out_path}'\n",
    "    shell_do(keep_train_cmd)\n",
    "\n",
    "    keep_test_cmd = f'plink2 --bfile {out_path} --keep {test_out_path}_ids.txt --make-bed --out {test_out_path}'\n",
    "    shell_do(keep_test_cmd)\n",
    "    \n",
    "    # loop through diseases\n",
    "    for disease in diseases:\n",
    "        # isolate correct SNPs and write to txt\n",
    "        assoc_disease = assoc[assoc['DISEASE'] == disease.upper()]\n",
    "        assoc_disease[['ID','ALLELE','BETA']].to_csv(f'{out_path}_{disease}_assoc_ids.txt', sep='\\t', index=False, header=False)\n",
    "        \n",
    "        # run train and test prs\n",
    "        score_train_cmd = f'plink2 --bfile {train_out_path} --score {out_path}_{disease}_assoc_ids.txt --out {train_out_path}'\n",
    "        shell_do(score_train_cmd)\n",
    "\n",
    "        score_test_cmd = f'plink2 --bfile {test_out_path} --score {out_path}_{disease}_assoc_ids.txt --out {test_out_path}'\n",
    "        shell_do(score_test_cmd)\n",
    "        \n",
    "        train_prs = pd.read_csv(f'{train_out_path}.sscore', sep='\\s+')\n",
    "        train_prs = train_prs.rename({'#FID':'ID','SCORE1_AVG':f'SCORE1_AVG_{disease}'}, axis=1)\n",
    "    \n",
    "        test_prs = pd.read_csv(f'{test_out_path}.sscore', sep='\\s+')\n",
    "        test_prs = test_prs.rename({'#FID':'ID','SCORE1_AVG':f'SCORE1_AVG_{disease}'}, axis=1)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        train_prs[f'PRS_STD_{disease}'] = scaler.fit_transform(np.asarray(train_prs[f'SCORE1_AVG_{disease}']).reshape(-1,1))\n",
    "        test_prs[f'PRS_STD_{disease}'] = scaler.transform(np.asarray(test_prs[f'SCORE1_AVG_{disease}']).reshape(-1,1))\n",
    "        \n",
    "        full_prs = pd.concat([train_prs, test_prs], axis=0)\n",
    "        # full_prs[f'PRS_STD_{disease}'] = scaler.fit_transform(np.asarray(full_prs[f'SCORE1_AVG_{disease}']).reshape(-1,1))\n",
    "        \n",
    "        train_full = train_full.merge(full_prs[['ID',f'PRS_STD_{disease}']], how='inner', on=['ID'])\n",
    "        test_full = test_full.merge(full_prs[['ID',f'PRS_STD_{disease}']], how='inner', on=['ID'])\n",
    "        \n",
    "        # plot\n",
    "        plt.hist(full_prs[f'PRS_STD_{disease}'])\n",
    "        plt.title(f'PRS Distribution - {disease}')\n",
    "        plt.xlabel('Standardized PRS')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        \n",
    "    return train_full, test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bc05d-05d1-48dd-9218-d71538c7b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disease_specific_prs_regression(train_data, test_data, data_type, standardize=False):\n",
    "    wd = f'insert_path'\n",
    "    \n",
    "    # copy of train and test data\n",
    "    train_data_copy = train_data.copy(deep=True)\n",
    "    test_data_copy = test_data.copy(deep=True)\n",
    "    \n",
    "    full_data_copy = pd.concat([train_data, test_data], axis=0)\n",
    "    \n",
    "    # set up some data lists\n",
    "    coef_data = []\n",
    "    std_data = []\n",
    "    or_data =[]\n",
    "    z_data = []\n",
    "    p_data = []\n",
    "    prs_mean_data = []\n",
    "    prs_std_data = []\n",
    "    \n",
    "    # prs col\n",
    "    prs_score = 'PRS_STD'\n",
    "    \n",
    "    # disease list\n",
    "    diseases = ['ad','pd','als','lbd','ftd']\n",
    "    \n",
    "    # get cluster cols\n",
    "    cluster_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'cluster_' in column:\n",
    "            cluster_cols.append(column)\n",
    "     \n",
    "    # get comp cols\n",
    "    comp_cols = []\n",
    "    for column in train_data_copy.columns:\n",
    "        if 'COMP' in column:\n",
    "            comp_cols.append(column)\n",
    "            \n",
    "    # set up cluster regression\n",
    "    if data_type == 'clusters':\n",
    "        cols = cluster_cols\n",
    "        axis = 'Cluster Membership'\n",
    "    # set up comp regression\n",
    "    else:\n",
    "        cols = comp_cols\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            train_data_copy[comp_cols] = scaler.fit_transform(train_data_copy[comp_cols])\n",
    "            test_data_copy[comp_cols] = scaler.transform(test_data_copy[comp_cols])\n",
    "            axis = 'Standardized Component'\n",
    "        else:\n",
    "            axis = 'Component'\n",
    "    \n",
    "    # loop through diseases\n",
    "    for disease in diseases:\n",
    "        # set up row data lists\n",
    "        coef_row_data = []\n",
    "        std_row_data = []\n",
    "        or_row_data =[]\n",
    "        z_row_data = []\n",
    "        p_row_data = []\n",
    "        prs_mean_row_data = []\n",
    "        prs_std_row_data = []\n",
    "        \n",
    "        # loop through columns\n",
    "        for col in cols:\n",
    "            # formula\n",
    "            formula = (f'{col} ~ {prs_score}_{disease}')\n",
    "            \n",
    "            if data_type == 'clusters':\n",
    "                model = logit(formula=formula, data=train_data_copy).fit(disp=0)\n",
    "                pred = model.predict(test_data_copy[f'{prs_score}_{disease}'])\n",
    "                stat_var = 'z'\n",
    "            else:\n",
    "                model = ols(formula=formula, data=train_data_copy).fit(disp=0)\n",
    "                pred = model.predict(test_data_copy[f'{prs_score}_{disease}'])\n",
    "                stat_var = 't'\n",
    "\n",
    "            results = pd.read_html(model.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
    "            coef_row_data.append(results.iloc[1]['coef'])\n",
    "            std_row_data.append(results.iloc[1]['std err'])\n",
    "            or_row_data.append(np.exp(results.iloc[1]['coef']))\n",
    "            z_row_data.append(results.iloc[1][f'{stat_var}'])\n",
    "            p_row_data.append(model.pvalues.loc[f'{prs_score}_{disease}'])\n",
    "            \n",
    "            stat, pval = ttest_1samp(full_data_copy[full_data_copy[col] == 1][f'{prs_score}_{disease}'], 0)\n",
    "            mean = full_data_copy[full_data_copy[col] == 1][f'{prs_score}_{disease}'].mean()\n",
    "            std = full_data_copy[full_data_copy[col] == 1][f'{prs_score}_{disease}'].std()\n",
    "            \n",
    "            if pval < 0.05:\n",
    "                prs_mean_row_data.append(f'{mean} ({std})*')\n",
    "            else:\n",
    "                prs_mean_row_data.append(f'{mean} ({std})')\n",
    "\n",
    "            # prs_mean_row_data.append(full_data[full_data[col] == 1][f'{prs_score}_{disease}'].mean())\n",
    "            prs_std_row_data.append(full_data_copy[full_data_copy[col] == 1][f'{prs_score}_{disease}'].std())\n",
    "        \n",
    "        coef_data.append(coef_row_data)\n",
    "        std_data.append(std_row_data)\n",
    "        or_data.append(or_row_data)\n",
    "        z_data.append(z_row_data)\n",
    "        p_data.append(p_row_data)\n",
    "        prs_mean_data.append(prs_mean_row_data)\n",
    "        prs_std_data.append(prs_std_row_data)\n",
    "    \n",
    "    \n",
    "    # dataframe heatmap data and plot\n",
    "    coef_data = pd.DataFrame(np.transpose(coef_data), index=cols, columns=diseases)\n",
    "    std_data = pd.DataFrame(np.transpose(std_data), index=cols, columns=diseases)\n",
    "    or_data = pd.DataFrame(np.transpose(or_data), index=cols, columns=diseases)\n",
    "    z_data = pd.DataFrame(np.transpose(z_data), index=cols, columns=diseases)\n",
    "    p_data = pd.DataFrame(np.transpose(p_data), index=cols, columns=diseases)\n",
    "    prs_mean_data = pd.DataFrame(np.transpose(prs_mean_data), index=cols, columns=diseases)\n",
    "    prs_std_data = pd.DataFrame(np.transpose(prs_std_data), index=cols, columns=diseases)\n",
    "    \n",
    "    print('OR')\n",
    "    print(or_data)\n",
    "    print('BETA')\n",
    "    print(coef_data)\n",
    "    print('SE')\n",
    "    print(std_data)\n",
    "    print('P')\n",
    "    print(p_data)\n",
    "    print('Mean PRS')\n",
    "    print(prs_mean_data)\n",
    "    # print('STD PRS')\n",
    "    # print(prs_std_data)\n",
    "    \n",
    "    # plot_heatmap(coef_data, f'{axis} vs. Disease-Specific PRS Score - Coefficients', 'Disease-Specific PRS Score', axis)\n",
    "    # plot_heatmap(or_data, f'{axis} vs. Disease-Specific PRS Score - OR', 'Disease-Specific PRS Score', axis)\n",
    "    # plot_heatmap(z_data, f'{axis} vs. Disease-Specific PRS Score - Z scores', 'Disease-Specific PRS Score', axis)\n",
    "    # plot_heatmap(p_data, f'{axis} vs. Disease-Specific PRS Score - P values', 'Disease-Specific PRS Score', axis)\n",
    "    \n",
    "    for row in range(coef_data.shape[0]):\n",
    "        for col in range(coef_data.shape[1]):\n",
    "            coef_data.iloc[row,col] = f'{coef_data.iloc[row,col]} ({std_data.iloc[row,col]})'\n",
    "    \n",
    "    coef_data.to_csv(f'{wd}/analysis/results/disease_specific_regression_coef.csv', sep=',', index=True, header=True)\n",
    "    p_data.to_csv(f'{wd}/analysis/results/disease_specific_regression_pval.csv', sep=',', index=True, header=True)\n",
    "    # print(coef_data)\n",
    "    # print(p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9675299b-cce7-4b49-9591-e81aa094b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_counts(data, fname):\n",
    "    wd = f'insert_path'\n",
    "    \n",
    "    # num samples\n",
    "    num_samples = data.shape[0]\n",
    "    \n",
    "    # get cluster cols\n",
    "    cols = ['Disease','Overall']\n",
    "    cluster_cols = []\n",
    "    for col in data.columns:\n",
    "        if 'cluster_' in col:\n",
    "            num = col.split('_')[1]\n",
    "            cols.append(f'C{num}')\n",
    "            cluster_cols.append(col)\n",
    "    \n",
    "    # set up data frame\n",
    "    disease_cluster_representation = pd.DataFrame(columns=cols)\n",
    "    disease_cluster_representation['Disease'] = ['ad','pd','als','lbd','ftd']\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    # get overall disease value counts\n",
    "    data_dict['Overall'] = dict(round(data['pheno'].value_counts(normalize=True), 5))\n",
    "    \n",
    "    # loop cluster cols\n",
    "    for i in range(len(cluster_cols)):\n",
    "        # get disease value counts for each cluster\n",
    "        data_cluster = data[data[cluster_cols[i]] == 1]\n",
    "        data_dict[f'C{i}'] = dict(round(data_cluster['pheno'].value_counts(normalize=True), 5))\n",
    "    \n",
    "    # fill out disease cluster rep df\n",
    "    for cluster in data_dict:\n",
    "        col_data = []\n",
    "        for disease in disease_cluster_representation['Disease']:\n",
    "            if disease in list(data_dict[cluster].keys()):\n",
    "                col_data.append(data_dict[cluster][disease])\n",
    "            else:\n",
    "                col_data.append(0)\n",
    "        disease_cluster_representation[cluster] = col_data\n",
    "        \n",
    "    print(disease_cluster_representation.shape)\n",
    "    \n",
    "    # z-test for proportions\n",
    "    for row in range(disease_cluster_representation.shape[0]):\n",
    "        print(disease_cluster_representation.iloc[row, 0])\n",
    "        for col in range(disease_cluster_representation.shape[1]):\n",
    "            if col > 1:\n",
    "                cluster_membership_overall = disease_cluster_representation.iloc[row, 1] * num_samples\n",
    "                cluster_membership_row = disease_cluster_representation.iloc[row, col] * num_samples\n",
    "                membership = np.array([cluster_membership_overall, cluster_membership_row])\n",
    "                samples = np.array([num_samples, num_samples])\n",
    "                stat, p_val = proportions_ztest(count=membership, nobs=samples, alternative='two-sided')\n",
    "                if p_val < 0.05:\n",
    "                    disease_cluster_representation.iloc[row, col] = f'{disease_cluster_representation.iloc[row, col]}*'\n",
    "                print(f'Overall vs. C{col-2}: z_stat={round(stat, 3)}, p_value={p_val}')\n",
    "        print()\n",
    "                \n",
    "    # print and write to file\n",
    "    print(disease_cluster_representation)\n",
    "    disease_cluster_representation.to_csv(f'{wd}/analysis/results/{fname}', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b0bf7-8884-4c7e-8930-bfdc0aa8ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prs_distributions_by_disease(full_data):\n",
    "    # isolate each cluster\n",
    "    cluster_0 = full_data[full_data['cluster'] == 0]\n",
    "    cluster_1 = full_data[full_data['cluster'] == 1]\n",
    "    cluster_2 = full_data[full_data['cluster'] == 2]\n",
    "    \n",
    "    diseases = ['ad','pd','als','lbd','ftd']\n",
    "    \n",
    "    fig, axs = plt.subplots(5)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.35)\n",
    "    \n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(18)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for disease in diseases:\n",
    "        sns.kdeplot(data=full_data, x=f'PRS_STD_{disease}', hue='cluster', palette='bright', common_norm=False, ax=axs[i])\n",
    "        axs[i].set_xlabel(f'{disease.upper()} PRS')\n",
    "        axs[i].set_title(f'{disease.upper()} PRS Distributions')\n",
    "        axs[i].axvline(x=0, c='black')\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    fig.show()\n",
    "    plt.savefig('figures/prs_distributions_per_disease.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0de97-adc0-4867-b87d-ca40d4e66281",
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_cmd = f'swarm -f get_umap_params.swarm -g 200 --time 10-00:00:00 --module python/3.9'\n",
    "shell_do(swarm_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce759b9-7dc1-4577-8274-271f6544ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_cmd = f'swarm -f cluster_consistency.swarm -g 200 --time 10-00:00:00 --module python/3.9'\n",
    "shell_do(swarm_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12116dc6-100d-4d8a-8cea-9a78e2aa4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 'insert_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d73fb-4ce3-4368-9632-3c487d0cd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_path = f'{wd}/processing/adjustment/downsampled/downsampled_gwas5e08_ADJUSTED10PCs.csv'\n",
    "pheno_path = f'{wd}/merged_genotypes/all_cohorts_phenotype.txt'\n",
    "plink_path = f'{wd}/merged_genotypes/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea04ca-186a-4b35-a7db-94325326e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted = pd.read_csv(adjusted_path, sep=',')\n",
    "print(adjusted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a430d4-1b35-47d1-8650-37cb7e631690",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rs7412' in adjusted.columns)\n",
    "print('rs429358' in adjusted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532bd17-857f-421f-b75f-1bdaeb04ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pd.read_csv(pheno_path, sep='\\s+', header=None)\n",
    "pheno.columns = ['ID','IID','PHENO','COHORT']\n",
    "pheno = pheno.drop_duplicates(subset=['ID','IID'], ignore_index=True)\n",
    "print(pheno.shape)\n",
    "print(pheno['PHENO'].value_counts())\n",
    "print(pheno['COHORT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ddd73-ce3b-4021-b2e8-70e1a28a9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = adjusted.merge(pheno[['ID','PHENO','COHORT']], how='inner', on=['ID'])\n",
    "cases = cases.drop_duplicates(ignore_index=True)\n",
    "print(cases.shape)\n",
    "print(cases['PHENO'].value_counts())\n",
    "print(cases['COHORT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b785d6-2a14-47c5-884e-160040b4b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = None\n",
    "X, y = set_up(cases, disease)\n",
    "col_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe96ee4-f50f-4b0e-859d-ea17572cb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4444)\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n",
    "\n",
    "train_id = X_train['ID']\n",
    "pd.concat([train_id, train_id], axis=1).to_csv('train_id.txt', sep='\\t', index=False, header=False)\n",
    "test_id = X_test['ID']\n",
    "pd.concat([test_id, test_id], axis=1).to_csv('test_id.txt', sep='\\t', index=False, header=False)\n",
    "train_cohort = X_train['COHORT']\n",
    "test_cohort = X_test['COHORT']\n",
    "X_train = X_train.drop(columns=['ID','COHORT'], axis=1)\n",
    "X_test = X_test.drop(columns=['ID','COHORT'], axis=1)\n",
    "y_cases = np.append(y_train, y_test)\n",
    "y_ids = np.append(train_id, test_id)\n",
    "y_cohorts = np.append(train_cohort, test_cohort)\n",
    "\n",
    "print(train_id.head())\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16220ef6-6056-4eec-8fa2-41dfca9572f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if disease:\n",
    "    prefix = f'{wd}/analysis/bootstrap/cluster_membership_df_100_rs_None_*_{disease}.txt'\n",
    "else:\n",
    "    prefix = f'{wd}/analysis/bootstrap/cluster_membership_df_100_rs_None_*.txt'\n",
    "\n",
    "full_clusters = get_permuted_clusters(prefix, y_ids, disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75c570-d546-4916-a57f-8fcc02f522ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_umap, X_test_umap = umap_transform(X_train, X_test, y_cases, 2.75, 0.75, seed=4444, cohorts=None)\n",
    "X_train_umap = pd.DataFrame(X_train_umap, columns=['UMAP1','UMAP2','UMAP3'])\n",
    "X_test_umap = pd.DataFrame(X_test_umap, columns=['UMAP1','UMAP2','UMAP3'])\n",
    "train_id = train_id.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_train_umap['ID'] = train_id\n",
    "X_train_umap['pheno'] = y_train\n",
    "test_id = test_id.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "X_test_umap['ID'] = test_id\n",
    "X_test_umap['pheno'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf303eac-5cef-4453-a071-e63247724df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters = X_train_umap.merge(full_clusters, how='inner', on=['ID'])\n",
    "test_clusters = X_test_umap.merge(full_clusters, how='inner', on=['ID'])\n",
    "\n",
    "full_clusters = pd.concat([train_clusters, test_clusters], axis=0, ignore_index=True)\n",
    "full_clusters['cluster'] = full_clusters['cluster'].astype(str)\n",
    "plot_3d(full_clusters, 'cluster', 'figures/cluster.html', x='UMAP1', y='UMAP2', z='UMAP3', title='UMAP Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd807c5-20ff-450e-a1ba-d5e5118084bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = expand_cluster_data(train_clusters)\n",
    "test_data = expand_cluster_data(test_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914522d3-df47-45da-b06c-71692e0037ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = disease_regression(train_data, test_data, 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380256a5-c08e-48c6-8bb0-1dbecc71f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_snp_assoc_path = f'{wd}/merged_genotypes/significance/gwas_common_snps_annovar_related_prune_eur_5e-08_snp_disease_genes.csv'\n",
    "gene_list_path = f'{wd}/analysis/gene_list.csv'\n",
    "extract_geno_path = f'{wd}/analysis/prs/prs_5e-08'\n",
    "train_full, test_full = get_full_data(X_train, X_test, train_data, test_data)\n",
    "train_full, test_full, col_names_full = change_col_names(train_full, test_full, disease_snp_assoc_path, gene_list_path, extract_geno_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c148e3b-0b21-4e33-b637-03f58547b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_snps_clusters = snp_regression(train_full, test_full, 'clusters')\n",
    "print(top_snps_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580405e3-b9af-4a07-b422-939778f78c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_only_geno_path = f'{wd}/merged_genotypes/significance/cases_only/gwas_common_snps_annovar_related_prune_eur_cases_5e-08'\n",
    "assoc_path = f'{wd}/merged_genotypes/significance/gwas_common_snps_annovar_related_prune_eur_5e-08_assoc.txt'\n",
    "prs_out_path = f'{wd}/analysis/prs/prs_5e-08'\n",
    "train_full, test_full = calculate_disease_specific_prs(adjusted=adjusted, train_full=train_full, test_full=test_full, geno_path=cases_only_geno_path, assoc_path=assoc_path, out_path=prs_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec634d-0625-401e-ac67-03f2262e0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_specific_prs_regression(train_data=train_full, test_data=test_full, data_type='clusters', standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f745438-3691-4f5e-9b53-dae93545cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.DataFrame(np.append(train_full, test_full, axis=0))\n",
    "full_data.columns = train_full.columns\n",
    "print(full_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24563e-2af3-48c2-9a58-b3725529bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_counts(full_data, 'disease_cluster_representation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c93e7-e393-4c8d-a4bf-0ecb72c20174",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_data[full_data['cluster_2'] == 1]['pheno'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a49d2-381a-4c7a-8b95-cad7f4300244",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prs_distributions_by_disease(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54681712-c71e-4c24-b40f-18fdd13fa77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_path = f'{wd}/processing/adjustment/downsampled/downsampled_gwas5e08_ADJUSTED10PCs.csv'\n",
    "adjusted = pd.read_csv(adjusted_path, sep=',')\n",
    "\n",
    "snp_set = pd.DataFrame(adjusted.columns)\n",
    "snp_set.columns = ['SNP']\n",
    "\n",
    "disease_snp_assoc_path = f'{wd}/merged_genotypes/significance/gwas_common_snps_annovar_related_prune_eur_5e-08_snp_disease_genes.csv'\n",
    "disease_snp_assoc = pd.read_csv(f'{disease_snp_assoc_path}', sep=',')\n",
    "disease_snp_assoc.columns = ['D1','D2','SNP','GENE']\n",
    "\n",
    "snps = snp_set.merge(disease_snp_assoc, how='inner', on=['SNP'])\n",
    "\n",
    "snps.to_csv(f'{wd}/analysis/snp_annotations.csv', sep=',', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db58d06-4fef-4aee-b83a-6f0eafb8eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = []\n",
    "avg_num_in_c0 = []\n",
    "\n",
    "for i in range(15):\n",
    "    bootstrap = pd.read_csv(f'{wd}/analysis/bootstrap/cluster_membership_df_100_rs_None_{i+1}.txt', sep='\\s+')\n",
    "    \n",
    "    num_in_c0 = []\n",
    "    num_in_c0_this_split = []\n",
    "\n",
    "    for col in bootstrap:\n",
    "        if (col != 'ID') and (col != 'pheno'):\n",
    "            num_clusters.append(len(bootstrap[col].value_counts()))\n",
    "            num_in_c0.append(bootstrap[col].value_counts()[0])\n",
    "            num_in_c0_this_split.append(len(bootstrap[col].value_counts()))\n",
    "\n",
    "    avg_num_in_c0.append(np.mean(num_in_c0))\n",
    "    # print(i+1)\n",
    "    # print(pd.Series(num_in_c0_this_split).value_counts())\n",
    "    \n",
    "\n",
    "split_index = np.arange(1, 16)\n",
    "\n",
    "avg_num_in_c0_data = {'Split':split_index, 'Avg. in C0':avg_num_in_c0}\n",
    "avg_num_in_c0_df = pd.DataFrame(avg_num_in_c0_data)\n",
    "print(avg_num_in_c0_df)\n",
    "print()\n",
    "print(pd.Series(num_clusters).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8050ccb-5d1f-4099-ba7e-0e8174cbfe65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
