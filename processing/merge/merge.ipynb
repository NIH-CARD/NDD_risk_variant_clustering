{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07a08b-3c6f-46ba-8a92-1a8bfdfa1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "from biothings_client import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b29be8-cc14-49cb-92f6-07b22c45fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell_do(command, log=False, return_log=False):\n",
    "    print(f'Executing: {(\" \").join(command.split())}', file=sys.stderr)\n",
    "\n",
    "    res=subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    if log:\n",
    "        print(res.stdout.decode('utf-8'))\n",
    "    if return_log:\n",
    "        return(res.stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513daf3-8c9f-4f15-98f9-ae8c13b4d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path_dict, merge_bim, common_snps_out_path):\n",
    "    # write common SNPs to txt file\n",
    "    merge_bim[['ID']].to_csv(common_snps_out_path, sep='\\t', index=False, header=False)\n",
    "    \n",
    "    # extracting common SNPs from each cohort \n",
    "    for cohort in path_dict:\n",
    "        extract_cmd = f\"plink2 --bfile {path_dict[cohort]['geno']} --extract {common_snps_out_path} --make-bed --out {path_dict[cohort]['out']}\"\n",
    "        shell_do(extract_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d0c93-1052-4a43-9fed-eb17124238a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(path_dict, merge_list_out_path, out_path):\n",
    "    # write out paths to txt file for merging\n",
    "    with open (merge_list_out_path, 'w') as f:\n",
    "        for cohort in path_dict:\n",
    "            f.write(f\"{path_dict[cohort]['out']}\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    # bash script for merging\n",
    "    with open('merge_genotypes.sh', 'w') as f:\n",
    "        f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write('module load plink/1.9\\n')\n",
    "        f.write(f'plink --merge-list {merge_list_out_path} --make-bed --out {out_path}\\n')\n",
    "        f.close()\n",
    "    \n",
    "    # swarm script\n",
    "    with open('merge_genotypes.swarm', 'w') as f:\n",
    "        f.write('bash merge_genotypes.sh')\n",
    "        f.close()\n",
    "        \n",
    "    # queue swarm job\n",
    "    shell_do('swarm -f merge_genotypes.swarm -g 200 --time 12:00:00 --module plink/1.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bc4df-0e7b-4ba6-9888-d99f4fed890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phenotype_file(path_dict, geno_path, out_path):\n",
    "    # read merged fam\n",
    "    fam = pd.read_csv(f'{geno_path}.fam', sep='\\s+', header=None)\n",
    "    fam.columns = ['FID','IID','PAT','MAT','SEX','PHENO']\n",
    "    print(fam.shape)\n",
    "    \n",
    "    # set up df\n",
    "    total_merge = pd.DataFrame(columns=['FID','IID','PHENOTYPE','COHORT'])\n",
    "\n",
    "    # loop through cohorts\n",
    "    for cohort in path_dict: \n",
    "        # merge fam files and set cohort variable\n",
    "        merge = path_dict[cohort]['fam'].merge(fam[['FID','IID']], how='inner', on=['FID','IID'])\n",
    "        merge['COHORT'] = cohort\n",
    "\n",
    "        # get pheno based on disease\n",
    "        if cohort == 'FTD':\n",
    "            merge['PHENOTYPE'] = np.where(merge['PHENO'] == 1, 'control', 'ftd')\n",
    "            total_merge = total_merge.append(merge[['FID','IID','PHENOTYPE','COHORT']])\n",
    "\n",
    "        elif cohort == 'LBD':\n",
    "            merge['PHENOTYPE'] = np.where(merge['PHENO'] == 1, 'control', 'lbd')\n",
    "            total_merge = total_merge.append(merge[['FID','IID','PHENOTYPE','COHORT']])\n",
    "\n",
    "        elif cohort == 'ALS':\n",
    "            merge['PHENOTYPE'] = np.where(merge['PHENO'] == 1, 'control', 'als')\n",
    "            total_merge = total_merge.append(merge[['FID','IID','PHENOTYPE','COHORT']])\n",
    "        \n",
    "        elif cohort == 'PD':\n",
    "            merge['PHENOTYPE'] = np.where(merge['PHENO'] == 1, 'control', 'pd')\n",
    "            total_merge = total_merge.append(merge[['FID','IID','PHENOTYPE','COHORT']])\n",
    "            \n",
    "        else:\n",
    "            merge['PHENOTYPE'] = np.where(merge['PHENO'] == 1, 'control', 'ad')\n",
    "            total_merge = total_merge.append(merge[['FID','IID','PHENOTYPE','COHORT']])\n",
    "\n",
    "    # drop any duplicates\n",
    "    total_merge = total_merge.drop_duplicates(subset=['FID','IID']) \n",
    "    print(total_merge.head())\n",
    "    print(total_merge.shape)\n",
    "    print(total_merge['PHENOTYPE'].value_counts())\n",
    "    print(total_merge['COHORT'].value_counts())\n",
    "    \n",
    "    total_merge.to_csv(f'{out_path}', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36853a49-8ee6-4b29-86f1-6ff9906b78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annovar_merge(annovar_dowload_file, geno_path, out_path):\n",
    "    # could run in swarm job\n",
    "    # get download file folder\n",
    "    wd = os.path.dirname(annovar_download_file)\n",
    "    \n",
    "    # unpack annovar in wd\n",
    "    unpack_cmd = f'tar xvfz {annovar_download_file} -C {wd}/'\n",
    "    shell_do(unpack_cmd)\n",
    "    annovar_folder_path = f'{wd}/annovar'\n",
    "    \n",
    "    # download hg38 - avsnp150\n",
    "    download_cmd = f'{annovar_folder_path}/annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avsnp150 {annovar_folder_path}/humandb/'\n",
    "    shell_do(download_cmd)\n",
    "    \n",
    "    # read in bim\n",
    "    bim = pd.read_csv(f'{geno_path}.bim', sep='\\s+')\n",
    "    bim.columns = ['CHR','ID','POS','BP','REF','ALT']\n",
    "    \n",
    "    # create annovar txt file\n",
    "    annovar_txt = pd.DataFrame(columns=['#CHROM','POS','ID','REF','ALT','QUAL','FILTER','INFO'])\n",
    "    annovar_txt['#CHROM'] = bim['CHR']\n",
    "    annovar_txt['POS'] = bim['BP']\n",
    "    annovar_txt['ID'] = bim['ID']\n",
    "    annovar_txt['REF'] = bim['ALT']\n",
    "    annovar_txt['ALT'] = bim['REF']\n",
    "    annovar_txt['QUAL'] = '.'\n",
    "    annovar_txt['FILTER'] = '.'\n",
    "    annovar_txt['INFO'] = 'PR'\n",
    "    annovar_txt.to_csv(f'{out_path}.txt', sep='\\t', index=False)\n",
    "    \n",
    "    # get RSIDs\n",
    "    convert_cmd = f'{annovar_folder_path}/table_annovar.pl {out_path}.txt {annovar_folder_path}/humandb/ -buildver hg38 --thread 1 -polish -out {out_path} -remove -protocol avsnp150 -operation f -nastring . -vcfinput'\n",
    "    shell_do(convert_cmd)\n",
    "    \n",
    "    # read in RSIDs\n",
    "    annovar_data_path = f'{out_path}.hg38_multianno.txt'\n",
    "    annovar_data = pd.read_csv(annovar_data_path, sep='\\s+')\n",
    "    \n",
    "    # merge with bim and get rid of SNPs with no RSID\n",
    "    merge = bim.merge(annovar_data[['avsnp150','Otherinfo6']], how='inner', left_on=['ID'], right_on=['Otherinfo6'])\n",
    "    merge_trimmed = merge[merge['avsnp150'] != '.']\n",
    "    \n",
    "    # write SNPs to text file and extract\n",
    "    snps_path = f'{out_path}_rsID.txt'\n",
    "    merge_trimmed[['ID']].to_csv(snps_path, sep='\\t', index=False, header=False)\n",
    "    extract_cmd = f'plink2 --bfile {geno_path} --extract {snps_path} --make-bed --out {out_path}'\n",
    "    shell_do(extract_cmd)\n",
    "    \n",
    "    # read in new bim file and convert ID so rsID\n",
    "    new_bim = pd.read_csv(f'{out_path}.bim', sep='\\s+', header=None)\n",
    "    new_bim.columns = ['CHR','ID','POS','BP','REF','ALT']\n",
    "    new_bim = new_bim.merge(annovar_data[['avsnp150','Otherinfo6']], how='inner', left_on=['ID'], right_on=['Otherinfo6'])\n",
    "    new_bim['ID'] = new_bim['avsnp150']\n",
    "    new_bim = new_bim.drop(columns=['avsnp150', 'Otherinfo6'], axis=1)\n",
    "    \n",
    "    # write to file\n",
    "    new_bim.to_csv(f'{out_path}.bim', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fb750-f42c-4565-a062-4bb52a6b0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merged_qc(bash_path, geno_path, out_path, env):\n",
    "    # write swarm file\n",
    "    with open(f'merge_qc.swarm', 'w') as f:\n",
    "        f.write(f'bash {bash_path} -i {geno_path} -o {out_path} -e {env}\\n')\n",
    "        f.close()\n",
    "    \n",
    "    # queue swarm job\n",
    "    shell_do('swarm -f merge_qc.swarm -g 200 --time 10-00:00:00 --module python/3.7,plink/1.9,GCTA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289e18b-1176-4d90-ae38-053c7e52825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merged_ancestry(bash_path, geno_path, ref_path, ref_labels_path, out_path, env):\n",
    "    # write swarm file\n",
    "    with open(f'merge_ancestry.swarm', 'w') as f:\n",
    "        pkl_path = f'{out_path}_umap_linearsvc_ancestry_model.pkl'\n",
    "        f.write(f'bash {bash_path} -g {geno_path} -r {ref_path} -l {ref_labels_path} -m {pkl_path} -o {out_path} -e {env}')\n",
    "        f.close()\n",
    "    # queue swarm job\n",
    "    shell_do('swarm -f merge_ancestry.swarm -g 200 --time 10-00:00:00 --module python/3.7,plink/1.9,admixture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f25b65-3b00-4383-8918-b0c0bff92678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_ancestry(geno_path, pred_labels_path, pca_path, out_path, n_pcs=2):\n",
    "    # read predicted_labels.txt file\n",
    "    pred_labels = pd.read_csv(pred_labels_path, sep='\\s+')\n",
    "    \n",
    "    # read project_new_pca.txt file and drop label column\n",
    "    pca = pd.read_csv(pca_path, sep='\\s+')\n",
    "    pca = pca.drop(columns=['label'], axis=1)\n",
    "    \n",
    "    # merge\n",
    "    merge = pred_labels.merge(pca, how='inner', on=['FID','IID'])\n",
    "    \n",
    "    # drop non-EUR cases\n",
    "    merge = merge[merge['label'] == 'EUR']\n",
    "    \n",
    "    # drop PC outliers (1+2 to start)\n",
    "    merge_trim_pca = merge.copy(deep=True)\n",
    "\n",
    "    for i in range(2):\n",
    "        print(f'PC{i+1}')\n",
    "        pc_col = f'PC{str(i+1)}'\n",
    "        q1 = np.quantile(merge[pc_col],0.25)\n",
    "        med = np.quantile(merge[pc_col],0.5)\n",
    "        q3 = np.quantile(merge[pc_col],0.75)\n",
    "        iq = q3-q1\n",
    "        lf = q1-(1.5*iq)\n",
    "        uf = q3+(1.5*iq)\n",
    "        merge_trim_pca = merge_trim_pca[(merge_trim_pca[pc_col]>lf) & (merge_trim_pca[pc_col]<uf)]\n",
    "        print(merge_trim_pca.shape)\n",
    "    \n",
    "    # write FID, IID to txt\n",
    "    merge_trim_pca[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=False)\n",
    "    \n",
    "    # plink --keep command\n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac160db-6a11-4494-a073-c16bd881be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gwas_hits(geno_path, ad_hits_path, pd_hits_path, als_hits_path, lbd_hits_path, out_path):\n",
    "    # read bim file\n",
    "    bim = pd.read_csv(f'{geno_path}.bim', sep='\\s+', header=None)\n",
    "    bim.columns = ['CHR','ID','LOC','BP','ALT','REF']\n",
    "    \n",
    "    # read hits files\n",
    "    ad_hits = pd.read_csv(ad_hits_path, sep='\\s+')\n",
    "    ad_hits['RSID'] = ad_hits['lead_SNP'].str.split('_').str[0]\n",
    "    pd_hits = pd.read_csv(pd_hits_path, sep=',')\n",
    "    pd_hits = pd_hits[pd_hits['GWAS'] == 'META5']\n",
    "    als_hits = pd.read_csv(als_hits_path, sep=',')\n",
    "    als_hits = als_hits.rename({'ID':'RSID'}, axis=1)\n",
    "    lbd_hits = pd.read_csv(lbd_hits_path, sep=',')\n",
    "    lbd_hits = lbd_hits.rename({'SNPS':'RSID'}, axis=1)\n",
    "    \n",
    "    # merge with bim, concat and drop duplicates\n",
    "    merge_ad = bim.merge(ad_hits[['RSID']], how='inner', left_on=['ID'], right_on=['RSID'])\n",
    "    merge_pd = bim.merge(pd_hits[['RSID']], how='inner', left_on=['ID'], right_on=['RSID'])\n",
    "    merge_als = bim.merge(als_hits[['RSID']], how='inner', left_on=['ID'], right_on=['RSID'])\n",
    "    merge_lbd = bim.merge(lbd_hits[['RSID']], how='inner', left_on=['ID'], right_on=['RSID'])\n",
    "    merge_hits = pd.concat([merge_ad,merge_pd,merge_als,merge_lbd], ignore_index=True)\n",
    "    merge_hits = merge_hits.drop_duplicates()\n",
    "    \n",
    "    merge_hits.to_csv(f'/data/CARD/projects/AD_Cluster/processing/sum_stats/combined.txt')\n",
    "    \n",
    "    # write IDs to txt and extract\n",
    "    merge_hits[['RSID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=False, header=False)\n",
    "    extract_cmd = f'plink2 --bfile {geno_path} --extract {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(extract_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba380f4-c59a-462e-a6a2-60611ffb4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gwas_variants(geno_path, ad_stats_path, pd_stats_path, als_stats_path, lbd_stats_path, ftd_hits_path, out_path):\n",
    "    # read bim file\n",
    "    bim = pd.read_csv(f'{geno_path}.bim', sep='\\s+', header=None)\n",
    "    bim.columns = ['CHR','ID','LOC','BP','ALT','REF']\n",
    "    bim['mergeID'] = bim['CHR'].astype(str) + ':' + bim['BP'].astype(str) \n",
    "    \n",
    "    # read stats files and make mergeID where necessary\n",
    "    ad_stats = pd.read_csv(ad_stats_path, sep='\\s+')\n",
    "    pd_stats = pd.read_csv(pd_stats_path, sep='\\s+')\n",
    "    pd_stats['mergeID'] = pd_stats['chr'].astype(str) + ':' + pd_stats['pos'].astype(str)\n",
    "    als_stats = pd.read_csv(als_stats_path, sep='\\s+')\n",
    "    als_stats['mergeID'] = als_stats['CHR'].astype(str) + ':' + als_stats['BP'].astype(str)\n",
    "    lbd_stats = pd.read_csv(lbd_stats_path, sep='\\s+')\n",
    "    ftd_stats = pd.read_csv(ftd_stats_path, sep='\\s+')\n",
    "    ftd_stats['mergeID'] = ftd_stats['Chr'].astype(str) + ':' + ftd_stats['Bp'].astype(str)\n",
    "    \n",
    "    # merge, drop, rename\n",
    "    merge_ad = bim.merge(ad_stats[['variant_id','p_value','effect_allele','beta']], how='inner', left_on=['ID'], right_on=['variant_id'])\n",
    "    merge_ad = merge_ad.drop(columns=['mergeID', 'variant_id'], axis=1)\n",
    "    merge_ad['disease'] = 'AD'\n",
    "    merge_pd = bim.merge(pd_stats[['mergeID','p','A1','b']], how='inner', on=['mergeID'])\n",
    "    merge_pd = merge_pd.rename({'p':'p_value','A1':'effect_allele','b':'beta'}, axis=1)\n",
    "    merge_pd = merge_pd.drop(columns=['mergeID'], axis=1)\n",
    "    merge_pd['disease'] = 'PD'\n",
    "    merge_als = bim.merge(als_stats[['mergeID','P','Allele1','Effect']], how='inner', on=['mergeID'])\n",
    "    merge_als = merge_als.rename({'P':'p_value','Allele1':'effect_allele','Effect':'beta'}, axis=1)\n",
    "    merge_als = merge_als.drop(columns=['mergeID'], axis=1)\n",
    "    merge_als['effect_allele'] = merge_als['effect_allele'].str.upper()\n",
    "    merge_als['disease'] = 'ALS'\n",
    "    merge_lbd = bim.merge(lbd_stats[['variant_id','p_value','effect_allele','beta']], how='inner', left_on=['ID'], right_on=['variant_id'])\n",
    "    merge_lbd = merge_lbd.drop(columns=['mergeID','variant_id'])\n",
    "    merge_lbd['disease'] = 'LBD'\n",
    "    merge_ftd = bim.merge(ftd_stats[['mergeID','P.value','Allele1','Beta']], how='inner', on=['mergeID'])\n",
    "    merge_ftd = merge_ftd.rename({'P.value':'p_value','Allele1':'effect_allele','Beta':'beta'}, axis=1)\n",
    "    merge_ftd = merge_ftd.drop(columns=['mergeID'], axis=1)\n",
    "    merge_ftd['effect_allele'] = merge_ftd['effect_allele'].str.upper()\n",
    "    merge_ftd['disease'] = 'FTD'\n",
    "    \n",
    "    # [5e-3,1e-3,5e-5,1e-5,5e-8]\n",
    "    \n",
    "    for val in [5e-8]:\n",
    "        # make out path\n",
    "        val_str = str(val)\n",
    "        out = f'{out_path}_{val_str}'\n",
    "        \n",
    "        # concatenate merged stats and drop by pval\n",
    "        concat = pd.concat([merge_ad,merge_pd,merge_als,merge_ftd,merge_lbd], ignore_index=True)\n",
    "        concat = concat[concat['p_value'] < val]\n",
    "        concat = concat.drop(columns=['p_value'], axis=1)\n",
    "        \n",
    "        # for PRS analysis\n",
    "        concat[['ID','effect_allele','beta','disease']].to_csv(f'{out}_assoc.txt', sep='\\t', header=False, index=False)\n",
    "        \n",
    "        # group same snps together\n",
    "        concat = concat.groupby(['CHR','ID','LOC','BP','ALT','REF']).agg(tuple).applymap(list).reset_index()\n",
    "        \n",
    "        # IDs to extract - appending all non-lead SNPs that are gene-associated\n",
    "        ids = concat[['ID']] \n",
    "        assoc_snps = {'rs7412':['AD'],'rs429358':['AD'],'rs75932628':['AD'],'rs143332484':['AD'],\n",
    "                      'rs2230288':['PD'],'rs75548401':['PD'],'rs76763715':['PD'],'rs34637584':['PD'],'rs1491942':['LBD']}\n",
    "        \n",
    "        needed_assoc_snps = []\n",
    "        \n",
    "        for key in list(assoc_snps.keys()):\n",
    "            if key not in ids.values:\n",
    "                needed_assoc_snps.append(key)\n",
    "        \n",
    "        ids = ids.append(pd.Series(needed_assoc_snps), ignore_index=True)\n",
    "        ids.to_csv(f'{out}_ids.txt', sep='\\t', index=False, header=False)\n",
    "        \n",
    "        # create snp disease association df\n",
    "        snp_disease = pd.DataFrame(concat['disease'].to_list())\n",
    "        snp_disease['ID'] = concat['ID']\n",
    "        \n",
    "        assoc_snps_disease = {}\n",
    "        \n",
    "        for i in range(snp_disease.shape[1]):\n",
    "            if i != (snp_disease.shape[1]-1):\n",
    "                assoc_snps_disease[i] = []\n",
    "            else:\n",
    "                assoc_snps_disease['ID'] = []\n",
    "        \n",
    "        for snp in assoc_snps:\n",
    "            if snp in needed_assoc_snps:\n",
    "                assoc_snps_disease[0].append(assoc_snps[snp][0])\n",
    "                for key in list(assoc_snps_disease.keys()):\n",
    "                    if (key != 0) and (key != 'ID'):\n",
    "                        assoc_snps_disease[key].append(np.nan)\n",
    "                assoc_snps_disease['ID'].append(snp)\n",
    "                \n",
    "        snp_disease = pd.concat([snp_disease,pd.DataFrame(assoc_snps_disease)], axis=0, ignore_index=True)\n",
    "        snp_disease.to_csv(f'{out}_snp_disease.csv', sep=',', header=False, index=False)\n",
    "        \n",
    "        # get gene associations\n",
    "        variants = list(snp_disease['ID'])\n",
    "        variant_map = {x : None for x in variants}\n",
    "        variant_client = get_client('variant')\n",
    "        rsIDs = variant_client.querymany(variants, scopes='dbsnp.rsid', fields='all', verbose=False)\n",
    "        for rsID in rsIDs:\n",
    "            query = rsID['query']\n",
    "            if 'dbsnp' in rsID:\n",
    "                dbsnp = rsID['dbsnp']\n",
    "                if 'gene' in dbsnp:\n",
    "                    gene = dbsnp['gene']\n",
    "                    if 'symbol' in gene and not variant_map[query]:\n",
    "                        variant_map[query] = gene['symbol']\n",
    "        genes = list(variant_map.values())\n",
    "        snp_disease['genes'] = genes\n",
    "        snp_disease.to_csv(f'{out}_snp_disease_genes.csv', sep=',', header=False, index=False)\n",
    "        \n",
    "        # extract gwas variants\n",
    "        extract_cmd = f'plink2 --bfile {geno_path} --extract {out}_ids.txt --make-bed --out {out}'\n",
    "        shell_do(extract_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3304e-acbe-40f2-b7a0-daf0fdcaa9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(geno_path, pheno_path, out_path):\n",
    "    # read fam file\n",
    "    fam = pd.read_csv(f'{geno_path}.fam', sep='\\s+', header=None)\n",
    "    fam.columns = ['FID','IID','PAT','MAT','SEX','PHENO']\n",
    "    \n",
    "    # read pheno file, remove controls\n",
    "    pheno = pd.read_csv(pheno_path, sep='\\s+', header=None)\n",
    "    pheno.columns = ['FID','IID','PHENO','COHORT']\n",
    "    pheno = pheno[pheno['PHENO'] != 'control']\n",
    "    \n",
    "    # merge\n",
    "    merge = fam.merge(pheno, how='inner', on=['FID','IID'])\n",
    "    print(merge.head())\n",
    "    \n",
    "    # sample n cases from each pheno\n",
    "    g = merge.groupby('PHENO_y', group_keys=False)\n",
    "    balanced = pd.DataFrame(g.apply(lambda x: x.sample(n=1000, random_state=42))).reset_index(drop=True)\n",
    "    \n",
    "    # write IDs to txt\n",
    "    balanced[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=False)\n",
    "    \n",
    "    # plink keep cmd\n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e3cfe-c80b-4d0b-97a0-423c65fbd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_controls(geno_path, pheno_path, out_path):\n",
    "    fam = pd.read_csv(f'{geno_path}.fam', sep='\\s+', header=None)\n",
    "    fam.columns = ['FID','IID','PAT','MAT','SEX','PHENO']\n",
    "    \n",
    "    pheno = pd.read_csv(f'{pheno_path}', sep='\\s+', header=None)\n",
    "    pheno.columns = ['FID','IID','PHENO','COHORT']\n",
    "    \n",
    "    merge = fam.merge(pheno, how='inner', on=['FID','IID'])\n",
    "    merge = merge[merge['PHENO_y'] != 'control']\n",
    "                      \n",
    "    merge[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=False)\n",
    "                      \n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26f86a-5ff9-4331-86ad-ca35848e352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project path\n",
    "wd = 'insert_path'\n",
    "\n",
    "# bash path\n",
    "bash_path = f'{wd}/processing/merge/merge_pipeline.sh'\n",
    "\n",
    "# pickle path\n",
    "path_dict_pkl_path = f'{wd}/processing/merge/path_dict.pkl'\n",
    "merge_pkl_path = f'{wd}/processing/merge/merge_bim.pkl'\n",
    "\n",
    "# conda enviornment (need jupyter and GenoTools installed)\n",
    "env = 'genotools'\n",
    "\n",
    "# out dir\n",
    "out_dir = f'{wd}/merged_genotypes'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# extract dir\n",
    "extract_dir = f'{out_dir}/extract'\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# stats dir\n",
    "stats_dir = f'{wd}/processing/sum_stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd471453-2c85-40aa-b30d-26c5bef01eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jg path\n",
    "jg_geno_path = f'{wd}/ROSMAPMayoRNAseqMSBB/joint_genotyping/lifted/jointGenotypingROSMAPMayoRNAseqMSBB_pheno_qc_lifted'\n",
    "jg_out_path = f'{extract_dir}/jointGenotyping_common_snps'\n",
    "\n",
    "# adni path\n",
    "adni_geno_path = f'{wd}/ADNI/genotypes/qc/ADNI_all_pheno_qc'\n",
    "adni_out_path = f'{extract_dir}/ADNI_common_snps'\n",
    "\n",
    "# ftd path\n",
    "ftd_geno_path = f'{wd}/FTD_LBD_ALS/ftd_genotypes/qc/merged_FTD_qc'\n",
    "ftd_out_path = f'{extract_dir}/FTD_common_snps'\n",
    "\n",
    "# lbd path\n",
    "lbd_geno_path = f'{wd}/FTD_LBD_ALS/lbd_genotypes/qc/merged_LBD_qc'\n",
    "lbd_out_path = f'{extract_dir}/LBD_common_snps'\n",
    "\n",
    "# als path\n",
    "als_geno_path = f'{wd}/FTD_LBD_ALS/als_genotypes/qc/merged_ALS_qc'\n",
    "als_out_path = f'{extract_dir}/ALS_common_snps'\n",
    "\n",
    "# pd path\n",
    "pd_geno_path = f'{wd}/AMP_PD/qc/amp_pd_pheno_qc'\n",
    "pd_out_path = f'{extract_dir}/amp_pd_common_snps'\n",
    "\n",
    "# adsp path \n",
    "adsp_geno_path = f'{wd}/ADSP/qc/adsp_formatted_normalized_pheno_new_ids'\n",
    "adsp_out_path = f'{extract_dir}/ADSP_common_snps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859cb5b-c4ed-440d-8374-b2b4c0005f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file if it exists\n",
    "if os.path.isfile(path_dict_pkl_path):\n",
    "    path_dict = joblib.load(path_dict_pkl_path)\n",
    "    \n",
    "else: # otherwise set it up\n",
    "    path_dict = {'JG':{}, 'ADNI':{}, 'FTD':{}, 'LBD':{}, 'ALS':{}, 'PD':{}, 'ADSP':{}}\n",
    "    \n",
    "    # adding to path dict\n",
    "    path_dict['JG']['geno'] = jg_geno_path\n",
    "    path_dict['JG']['out'] = jg_out_path\n",
    "    path_dict['ADNI']['geno'] = adni_geno_path\n",
    "    path_dict['ADNI']['out'] = adni_out_path\n",
    "    path_dict['FTD']['geno'] = ftd_geno_path\n",
    "    path_dict['FTD']['out'] = ftd_out_path\n",
    "    path_dict['LBD']['geno'] = lbd_geno_path\n",
    "    path_dict['LBD']['out'] = lbd_out_path\n",
    "    path_dict['ALS']['geno'] = als_geno_path\n",
    "    path_dict['ALS']['out'] = als_out_path\n",
    "    path_dict['PD']['geno'] = pd_geno_path\n",
    "    path_dict['PD']['out'] = pd_out_path\n",
    "    path_dict['ADSP']['geno'] = adsp_geno_path\n",
    "    path_dict['ADSP']['out'] = adsp_out_path\n",
    "    \n",
    "    # read bim files and add to dict\n",
    "    for cohort in path_dict:\n",
    "        path_dict[cohort]['bim'] = pd.read_csv(f\"{path_dict[cohort]['geno']}.bim\", sep='\\s+', header=None)\n",
    "        path_dict[cohort]['bim'].columns = ['CHR','ID','LOC','BP','ALT','REF']\n",
    "        print(cohort)\n",
    "        print(path_dict[cohort]['bim'].shape)\n",
    "    \n",
    "    # read fam files and add to dict\n",
    "    for cohort in path_dict:\n",
    "        path_dict[cohort]['fam'] = pd.read_csv(f\"{path_dict[cohort]['geno']}.fam\", sep='\\s+', header=None)\n",
    "        path_dict[cohort]['fam'].columns = ['FID','IID','PAT','MAT','SEX','PHENO']\n",
    "        print(cohort)\n",
    "        print(path_dict[cohort]['fam'].shape)\n",
    "    \n",
    "    # create pickle file\n",
    "    joblib.dump(path_dict, path_dict_pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180e75a-f761-4a97-8de9-8f0ffb9e1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file if it exists\n",
    "if os.path.isfile(merge_pkl_path):\n",
    "    merge_bim = joblib.load(merge_pkl_path)\n",
    "    print(merge_bim.shape)\n",
    "    \n",
    "else: # otherwise merge cohort bim files together\n",
    "    merge_bim = path_dict['JG']['bim'].merge(path_dict['ADNI']['bim'][['ID']], how='inner', on=['ID'])\n",
    "    print(merge_bim.shape)\n",
    "    for cohort in path_dict:\n",
    "        if cohort != 'JG' and cohort != 'ADNI':\n",
    "            merge_bim = merge_bim.merge(path_dict[cohort]['bim'][['ID']], how='inner', on=['ID'])\n",
    "            print(merge_bim.shape)\n",
    "            \n",
    "    # create pickle file\n",
    "    joblib.dump(merge_bim, merge_pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35744b3b-1632-4a72-9872-93ad4f0e9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_snps_out_path = f'{wd}/merged_genotypes/common_snps.txt'\n",
    "extract(path_dict, merge_bim, common_snps_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4189a6b-7173-4f19-a54c-39b2bd43883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_list_out_path = f'{wd}/merged_genotypes/merge_list.txt'\n",
    "merge_geno_path = f'{wd}/merged_genotypes/all_cohorts_common_snps'\n",
    "merge(path_dict, merge_list_out_path, merge_geno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca27181-ff01-4f94-a394-8aadef137559",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_out_path = f'{wd}/merged_genotypes/all_cohorts_phenotype.txt'\n",
    "make_phenotype_file(path_dict, merge_geno_path, phenotype_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4d2b0-51cb-43e2-b89f-5e2cf310f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "annovar_download_file = 'insert_path_to_annovar_gz_download_file'\n",
    "annovar_out_path = f'{merge_geno_path}_annovar'\n",
    "annovar_merge(annovar_download_file, merge_geno_path, annovar_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec2b9a-b9db-44ad-bc12-8b267f65c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_bash_path = f'{wd}/processing/merge/merge_qc.sh'\n",
    "qc_out_path = f'{wd}/merged_genotypes/qc/all_cohorts_common_snps_annovar_related_prune'\n",
    "run_merged_qc(qc_bash_path, annovar_out_path, qc_out_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb1bb8-2955-4934-8d94-6bf689c1585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestry_bash_path = f'{wd}/processing/merge/merge_ancestry.sh'\n",
    "ref_path = 'insert_ref_panel_path'\n",
    "ref_labels_path = 'insert_ref_label_path'\n",
    "ancestry_out_path = f'{wd}/merged_genotypes/ancestry/all_cohorts_common_snps_annovar_related_prune'\n",
    "run_merged_ancestry(ancestry_bash_path, qc_out_path, ref_path, ref_labels_path, ancestry_out_path, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9637b0-da97-4102-bdb0-c0328d54e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_path = f'{wd}/merged_genotypes/ancestry/all_cohorts_common_snps_annovar_related_prune_umap_linearsvc_predicted_labels.txt'\n",
    "pred_labels = pd.read_csv(pred_labels_path, sep='\\s+')\n",
    "print(pred_labels['label'].value_counts())\n",
    "adj_labels_path = f'{wd}/merged_genotypes/ancestry/all_cohorts_common_snps_annovar_related_prune_adjusted_labels.txt'\n",
    "adj_labels = pd.read_csv(adj_labels_path, sep='\\s+')\n",
    "print(adj_labels['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becabbd-c7be-4bce-9806-b02a438f1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_path = f'{wd}/merged_genotypes/ancestry/all_cohorts_common_snps_annovar_related_prune_umap_linearsvc_predicted_labels.txt'\n",
    "pca_path = f'{wd}/merged_genotypes/ancestry/all_cohorts_common_snps_annovar_related_prune_projected_new_pca.txt'\n",
    "ancestry_prune_out_path = f'{wd}/merged_genotypes/ancestry/all_cohorts_common_snps_annovar_related_prune_eur'\n",
    "remove_by_ancestry(qc_out_path, pred_labels_path, pca_path, ancestry_prune_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff222fcf-ed1f-429a-be97-3ebfce9423f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_hits_path = f'{wd}/processing/sum_stats/AD.loci.tsv'\n",
    "pd_hits_path = f'{wd}/processing/sum_stats/pd_gwas_risk_variants.csv'\n",
    "als_hits_path = f'{wd}/processing/sum_stats/als_gwas_risk_variants.csv'\n",
    "lbd_hits_path = f'{wd}/processing/sum_stats/lbd_gwas_risk_variants.csv'\n",
    "gwas_out_path = f'{wd}/merged_genotypes/gwas_hits_common_snps_annovar_related_prune_eur'\n",
    "extract_gwas_hits(ancestry_prune_out_path, ad_hits_path, pd_hits_path, als_hits_path, lbd_hits_path, gwas_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5552f2-6bbe-4587-a573-35f054cbc697",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_stats_path = f'{stats_dir}/Schwartzentruber_2021_lifted_hg38.txt'\n",
    "pd_stats_path = f'{stats_dir}/nallsEtAl2019_lifted_hg38.txt'\n",
    "als_stats_path = f'{stats_dir}/alsMetaSummaryStats_lifted_hg38.txt'\n",
    "lbd_stats_path = f'{stats_dir}/GCST90001390_buildGRCh38.tsv'\n",
    "ftd_stats_path = f'{stats_dir}/Meta-analysis.Matched.AllResults_lifted_hg38.txt'\n",
    "significance_out_path = f'{wd}/merged_genotypes/significance/gwas_common_snps_annovar_related_prune_eur'\n",
    "extract_gwas_variants(ancestry_prune_out_path, ad_stats_path, pd_stats_path, als_stats_path, lbd_stats_path, ftd_stats_path, significance_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de739e4e-481d-44ed-9995-b1960ad57335",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_only_out_path = f'{wd}/merged_genotypes/significance/cases_only/gwas_common_snps_annovar_related_prune_eur_cases_5e-08'\n",
    "remove_controls(f'{significance_out_path}_5e-08', phenotype_out_path, cases_only_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a15143-9361-42ad-9471-ca0b77775feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_geno_path = f'{significance_out_path}_5e-08'\n",
    "downsampled_out_path = f'{wd}/merged_genotypes/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled'\n",
    "downsample(significance_geno_path, phenotype_out_path, downsampled_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6333006-3784-4686-b8fd-4c4a7675a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = pd.read_csv(f'{downsampled_out_path}.fam', sep='\\s+', header=None)\n",
    "print(fam.head())\n",
    "print(fam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db596fc9-c8da-4b7b-8cca-5197c5e70c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bim = pd.read_csv(f'{downsampled_out_path}.bim', sep='\\s+', header=None)\n",
    "print(bim.head())\n",
    "print(bim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78307096-c37d-4996-be28-2d4d487b1c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
