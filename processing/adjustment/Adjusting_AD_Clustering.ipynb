{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46787ce2",
   "metadata": {},
   "source": [
    "## Adjust genotypes by APOE and PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c955175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import tables\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statistics\n",
    "import umap.umap_ as umap\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "from QC.utils import shell_do\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49c80c-bbfc-47b1-a144-fd8dbf44b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GenoML to construct the .h5 that will be used for adjusting (GenoML will normalize and munge the data as well)\n",
    "wd ='insert_path'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a66a5",
   "metadata": {},
   "source": [
    "## APOE prep (SKIP FOR NOW)\n",
    "\n",
    "Check imputation quality. If R2 > 0.95 then follow this script to get APOE genotype: https://github.com/neurogenetics/APOE_genotypes\n",
    "\n",
    "Then create a column that has 0 for no e4 alleles, 1 for e3/e4 and e1/e4, 2 for e4/e4.\n",
    "\n",
    "If R2 < 0.95, use dosage of rs429358 and rs7412 as 2 separate columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1095da4-8ec7-45a8-9812-663cb5f463c2",
   "metadata": {},
   "source": [
    "## PCA Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd917304-797b-4359-bfc0-dbf9c0f6a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using flashPCA      \n",
    "def adj_pca_flash(geno_path, out_path, dim, exclusion = None):\n",
    "    # Filter data (you can speed up things by adding --memory 119500 --threads 19 in PLINK)\n",
    "    if exclusion != None:\n",
    "        cmd_filter = f'plink --bfile {geno_path} --maf 0.05 --geno 0.01 --hwe 5e-6 --autosome --exclude {exclusion} --make-bed --out {out_path}'\n",
    "    else:\n",
    "        cmd_filter = f'plink --bfile {geno_path} --maf 0.05 --geno 0.01 --hwe 5e-6 --autosome --make-bed --out {out_path}'\n",
    "    \n",
    "    # Prune snps \n",
    "    cmd_prune = f'plink --bfile {out_path} --indep-pairwise 1000 10 0.02 --autosome --out {out_path}_pruned_data'\n",
    "    \n",
    "    # Extract pruned SNPs and only these variants will be used for PC calculation\n",
    "    cmd_exract = f'plink --bfile {out_path} --extract {out_path}_pruned_data.prune.in --make-bed --out {out_path}_pruned_snps'\n",
    "    \n",
    "    # run PCS using flashpca\n",
    "    flash_pca(f'{out_path}_pruned_snps', f'{out_path}_pca', dim = dim)\n",
    "    \n",
    "def adj_pca_fbash(geno_path, out_path, dim, exclusion = None):\n",
    "    \n",
    "    with open('PCA_prep_flash.py', 'w') as f:\n",
    "        #f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write('from flash_pca import flash_pca \\n')\n",
    "        f.write('from QC.utils import shell_do')\n",
    "        f.write('\\n \\n')\n",
    "        # Filter data (you can speed up things by adding --memory 119500 --threads 19 in PLINK)\n",
    "        if exclusion != None:\n",
    "            f.write(f'cmd1 = \"plink --bfile {geno_path} --maf 0.05 --geno 0.01 --hwe 5e-6 --autosome --exclude {exclusion} --make-bed --out {out_path}\" \\n')\n",
    "        else:\n",
    "            f.write(f'cmd1 = \"plink --bfile {geno_path} --maf 0.05 --geno 0.01 --hwe 5e-6 --autosome --make-bed --out {out_path}\" \\n')\n",
    "    \n",
    "        # Prune snps \n",
    "        f.write(f'cmd2 = \"plink --bfile {out_path} --indep-pairwise 1000 10 0.02 --autosome --out {out_path}_pruned_data\" \\n')\n",
    "    \n",
    "        # Extract pruned SNPs and only these variants will be used for PC calculation\n",
    "        f.write(f'cmd3 = \"plink --bfile {out_path} --extract {out_path}_pruned_data.prune.in --make-bed --out {out_path}_pruned_snps\" \\n \\n')\n",
    "    \n",
    "        f.write('cmd_list = [cmd1,cmd2,cmd3] \\n')\n",
    "        f.write('[shell_do(cmd) for cmd in cmd_list] \\n')\n",
    "        # Calculate/generate PCs based on pruned data set + loadings\n",
    "        f.write(f'flash_pca(\"{out_path}_pruned_snps\", \"{out_path}\", dim = {dim})')\n",
    "    f.close()\n",
    "    \n",
    "    # write swarm script\n",
    "    with open('PCA_prep_flash.swarm', 'w') as f:\n",
    "        f.write('python PCA_prep_flash.py')\n",
    "    f.close()\n",
    "        \n",
    "    swarm_cmd = f'swarm -f PCA_prep_flash.swarm -g 50 --time 24:00:00 --module plink/1.9.0-beta4.4,flashpca'\n",
    "    shell_do(swarm_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe18619-a612-4f9b-a936-14e224a49ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geno = f'{wd}/merged_genotypes/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled'\n",
    "out = f'{wd}/processing/adjustment/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled'\n",
    "exc = f'{wd}/processing/adjustment/hg38_exclusion_regions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67086d-79ca-46a8-b642-3a9a786668fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_pca_fbash(geno, out, dim = 10, exclusion = exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6329d27-d162-438c-a470-d1d0a22d5970",
   "metadata": {},
   "source": [
    "## Confounders file\n",
    "#### If using flashpca method for PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c1e63-95ad-467b-976e-1e1fbe7caaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eigenvecs = f'{wd}/processing/adjustment/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled.pcs'\n",
    "# create df\n",
    "eigen_df = pd.read_csv(pca_eigenvecs, sep = '\\s+')\n",
    "\n",
    "# modify df to proper format\n",
    "loadings_df_10 = eigen_df.iloc[:,1:]\n",
    "loadings_df_10.rename({'IID': 'ID'}, axis = 1, inplace = True) \n",
    "loadings_df_10\n",
    "\n",
    "# export out file \n",
    "loadings_df_10.to_csv(f'{wd}/processing/adjustment/downsampled/gwas_common_snps_10PC_LOADINGS.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0fe0c-e8d1-43b5-b441-707e4b8a488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in\n",
    "confounders = f'{wd}/processing/adjustment/downsampled/gwas_common_snps_10PC_LOADINGS.csv'\n",
    "confounders_df = pd.read_csv(confounders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b92ff-7590-4289-b2c1-ad8034e16c59",
   "metadata": {},
   "source": [
    "## Use GenoML to prep data for adjustment\n",
    "\n",
    "Directions for installing GenoML: https://github.com/GenoML/genoml2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db6b23-7bdf-4ef0-92ca-c66b67f92bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!genoml discrete supervised munge \\\n",
    "--geno /wd/merged_genotypes/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled \\\n",
    "--prefix /wd/processing/adjustment/downsampled/gwas_common_snps_related_prune_5e-08_downsampled \\\n",
    "--pheno /wd/processing/adjustment/downsampled/gwas_common_snps_annovar_related_prune_eur_5e-08_downsampled_pheno.csv \\\n",
    "--impute mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4538a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in munge file\n",
    "munged_data = f'{wd}/processing/adjustment/downsampled/gwas_common_snps_related_prune_5e-08_downsampled.dataForML.h5'\n",
    "target_data_df = pd.read_hdf(munged_data, 'dataForML')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b822a-16a7-482a-8bc1-735f31e6ddad",
   "metadata": {},
   "source": [
    "### ADJUSTMENT PREP AND RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9356d3d-34bb-4a9f-a9c7-62a32a14b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target columns file and export out\n",
    "cols = target_data_df.columns.tolist()[2:]\n",
    "f = open(f\"{wd}/processing/adjustment/downsampled/gwas_common_snps_related_prune_5e-08_downsampled_columns_5e_08_cases.txt\", \"w\")\n",
    "for var in cols:\n",
    "    f.write(var)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4712a-d263-492e-9e0a-4b4d87f0bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in target columns file and create df\n",
    "target_columns = f\"{wd}/processing/adjustment/downsampled/gwas_common_snps_related_prune_5e-08_downsampled_columns_5e_08_cases.txt\"\n",
    "target_column_df = pd.read_csv(target_columns, names=['TARGETS'])\n",
    "\n",
    "# Keep only intersecting feature names left in munged set (removed either because --gwas or std dev of 0 etc.)\n",
    "target_data_list = target_data_df.columns\n",
    "target_column_list = target_column_df['TARGETS'].tolist()\n",
    "intersecting_list = list(set(target_data_list).intersection(set(target_column_list)))\n",
    "\n",
    "target_column_df = pd.DataFrame(intersecting_list,columns=['TARGETS'])\n",
    "\n",
    "print(len(intersecting_list))\n",
    "print(intersecting_list[:10])\n",
    "print(target_data_df.head())\n",
    "print(target_data_df.shape)\n",
    "\n",
    "normalize_switch = 'yes' # Yep or nope to run the Z normalization of residuals.\n",
    "\n",
    "# Munging begins. First make feature lists for targets and confounders. Then merge datasets.\n",
    "\n",
    "target_list = list(target_column_df['TARGETS'])\n",
    "confounder_list = list(confounders_df.columns[1:])\n",
    "columns_to_keep_list = list(target_data_df.columns)\n",
    "\n",
    "adjustments_df = target_data_df.merge(confounders_df, how='inner', on='ID', suffixes=['', '_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ab4e9-830c-4e29-97da-7981960d9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we start the adjusting\n",
    "## First, make the formula pieces\n",
    "import time\n",
    "t0 = time.time()\n",
    "formula_for_confounders = ' + '.join(confounder_list)\n",
    "i = 1\n",
    "for target in target_list:\n",
    "    current_target = str(target)\n",
    "    print(f\"working on {current_target}. {i} out of {len(target_list)} targets complete. {round((i/len(target_list))*100,2)}% complete\")\n",
    "    current_formula = target + \" ~ \" + formula_for_confounders\n",
    "    #print(current_formula)\n",
    "    target_model = smf.ols(formula= current_formula, data=adjustments_df).fit()\n",
    "    if normalize_switch == 'yes':\n",
    "        adjustments_df['temp'] = pd.to_numeric(target_model.resid)\n",
    "        #print(type(adjustments_df['temp']))\n",
    "        mean_scalar = adjustments_df['temp'].mean()\n",
    "        sd_scalar = adjustments_df['temp'].std()\n",
    "        adjustments_df[current_target] = (adjustments_df['temp'] - mean_scalar)/sd_scalar\n",
    "        adjustments_df.drop(columns=['temp'], inplace=True)\n",
    "    \n",
    "    else:\n",
    "        adjustments_df[current_target] = pd.to_numeric(target_model.resid)\n",
    "    i += 1\n",
    "t1 = time.time()\n",
    "\n",
    "runtime = t0-t1/60\n",
    "\n",
    "print(f'Total run time was {runtime} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b738b27-811d-47ad-9fcd-e42a44245688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now some more munging to just extract columns that are in original dataset\n",
    "adjusted_df = adjustments_df[columns_to_keep_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6139b-1624-4f8f-9f7a-6fed0c334c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as an --addit file format\n",
    "tweaked_adj_df = adjusted_df.drop(columns=['PHENO'])\n",
    "tweaked_adj_df.to_csv(f\"{wd}/processing/adjustment/downsampled/downsampled_gwas5e08_ADJUSTED10PCs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb6f03-8ad7-49de-9f9d-3cf3a085fffa",
   "metadata": {},
   "source": [
    "### Modify IDs and SNP Position columns to avoid errors later on (ONLY IF YOU DONT HAVE RSID BUT HAVE POSITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e1c26-ac8b-4342-ad4f-92d0b2ff0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dfs to match up temp names with originals\n",
    "col_list = target_data_df.columns.tolist()\n",
    "key_df = pd.DataFrame() # holds SNP data\n",
    "key_df['Original'] = col_list\n",
    "\n",
    "# modify column names for SNPs\n",
    "edit_tl = ['ID', 'PHENO']\n",
    "for i in range(len(col_list[2:])):\n",
    "    new = f'var{i}'\n",
    "    edit_tl.append(new)\n",
    "key_df['temp'] = edit_tl\n",
    "\n",
    "# modify IDs\n",
    "id_df = pd.DataFrame()\n",
    "id_df['Original_id'] = target_data_df['ID']\n",
    "id_df['temp_id'] = range(len(target_data_df['ID'])) # temp ids\n",
    "\n",
    "# update confounders IDs to match\n",
    "confounders_df['ID'] = id_df['temp_id']\n",
    "\n",
    "# create dfs to match up temp names with originals\n",
    "col_list = target_data_df.columns.tolist()\n",
    "key_df = pd.DataFrame()\n",
    "key_df['Original'] = col_list\n",
    "\n",
    "edit_tl = ['ID','PHENO']\n",
    "\n",
    "\n",
    "for i in range(len(col_list[2:])):\n",
    "    new = f'var{i}'\n",
    "    edit_tl.append(new)\n",
    "    \n",
    "key_df['temp'] = edit_tl\n",
    "\n",
    "# modify IDs\n",
    "id_df = pd.DataFrame()\n",
    "id_df['Original_id'] = target_data_df['ID']\n",
    "id_df['temp_id'] = range(len(target_data_df['ID']))\n",
    "\n",
    "# replacing SNP names with temp name for adjustment\n",
    "target_data_df2 = target_data_df.set_axis(edit_tl, axis = 1)\n",
    "target_data_df2['ID'] = id_df['temp_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c44ab0-7e8e-4f8d-89fd-92422fff045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace names \n",
    "# extract correct labels\n",
    "replace_cols = target_data_df.columns.to_list()\n",
    "\n",
    "# replace with correct labels \n",
    "adjusted_df.columns = replace_cols\n",
    "adjusted_df.loc[:,'ID'] = id_df.loc[:,'Original_id']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
