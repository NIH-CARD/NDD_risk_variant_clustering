{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d996c8d-6dbc-42ca-b063-1a80116bcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b313fc2-ca3c-4a43-9b1c-e5f4e7f6024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell_do(command, log=False, return_log=False):\n",
    "    print(f'Executing: {(\" \").join(command.split())}', file=sys.stderr)\n",
    "\n",
    "    res=subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    if log:\n",
    "        print(res.stdout.decode('utf-8'))\n",
    "    if return_log:\n",
    "        return(res.stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f045da2-9e1f-4305-a9d5-d35e76e50fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vcf_to_pfile(seq_data_files, temp_vcf, out_files):\n",
    "    # write bash script\n",
    "    with open('vcf_to_pfile.sh', 'w') as f:\n",
    "        f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write('module load plink/2.3-alpha samtools\\n')\n",
    "        f.write(f'bcftools concat {seq_data_files} -o {temp_vcf}\\n')\n",
    "        f.write(f'plink2 --vcf {temp_vcf} --allow-extra-chr --double-id --make-pgen --out {out_files}\\n')\n",
    "        f.write(f'rm {temp_vcf}')\n",
    "        f.close()\n",
    "    \n",
    "    # write swarm script\n",
    "    with open('vcf_to_pfile.swarm', 'w') as f:\n",
    "        f.write('bash vcf_to_pfile.sh')\n",
    "        f.close()\n",
    "\n",
    "    # queue swarm job\n",
    "    swarm_cmd = f'swarm -f vcf_to_pfile.swarm -g 20 --time 12:00:00 --module plink/2.0_alpha_1_final,samtools'\n",
    "    shell_do(swarm_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba25ce1-58ca-44e4-88c9-71bb4c455db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfiles_to_bfiles(pfiles_path, bfiles_path):\n",
    "    # convert plink2 pfiles to plink binaries (all chr at once)\n",
    "    plink_cmd = f'plink2 --pfile {pfiles_path} --allow-extra-chr --chr 1-23 --make-bed --out {bfiles_path}'\n",
    "    shell_do(plink_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a6f61-ec8b-48bf-833a-e0d188f889f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bfiles(bfiles_path, out_path, num_chr):\n",
    "    # write individual chr paths to txt file\n",
    "    with open(f'{out_path}.txt', 'w') as f:\n",
    "        for i in range(num_chr):\n",
    "            bfiles_individual_path = bfiles_path.replace('*', str(i+1))\n",
    "            # reformatting bim avoids merge errors\n",
    "            reformat_bim(f'{bfiles_individual_path}.bim')\n",
    "            f.write(f'{bfiles_individual_path}\\n')\n",
    "        f.close()\n",
    "    \n",
    "    # write bash script\n",
    "    with open('merge_bfiles.sh', 'w') as f:\n",
    "        f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write('module load plink/1.9\\n')\n",
    "        f.write(f'plink --merge-list {out_path}.txt --make-bed --out {out_path}')\n",
    "        f.close()\n",
    "    \n",
    "    # write swarm script\n",
    "    with open('merge_bfiles.swarm', 'w') as f:\n",
    "        f.write('bash merge_bfiles.sh')\n",
    "        f.close()\n",
    "    \n",
    "    # queue swarm job\n",
    "    swarm_cmd = f'swarm -f merge_bfiles.swarm -g 1000 --partition largemem -t 64 --time 10-00:00:00 --module plink/1.9'\n",
    "    shell_do(swarm_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0839b-7461-46c1-b01e-a8c07a9e4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fam(fam, change_type=True, copy_iid=False):\n",
    "    # read fam file, name columns\n",
    "    fam = pd.read_csv(fam, sep='\\s+', header=None)\n",
    "    fam.columns = ['FID','IID','PAT','MAT','SEX','PHENO']\n",
    "    \n",
    "    # convert to string type for easy merging\n",
    "    if change_type:\n",
    "        fam['FID'] = fam['FID'].astype(str)\n",
    "        fam['IID'] = fam['IID'].astype(str)\n",
    "        \n",
    "    # make FID and IID the same\n",
    "    if copy_iid:\n",
    "        fam['FID'] = fam['IID'].copy()\n",
    "    \n",
    "    return fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459d19c-d147-4632-85ac-36021f5abaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_bim(bim_path):\n",
    "    # read bim file, name columns\n",
    "    bim = pd.read_csv(bim_path, sep='\\s+', header=None)\n",
    "    bim.columns = ['CHR','ID','LOC','BP','ALT','REF']\n",
    "    \n",
    "    # change to chr:basepair:ref:alt format and write to file\n",
    "    bim['ID'] = bim['CHR'].astype(str) + ':' + bim['BP'].astype(str) + ':' + bim['REF'] + ':' + bim['ALT']\n",
    "    bim.to_csv(bim_path, sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7357a85-478b-4cb1-9301-d778ca29169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ids(geno_path, ids_path):\n",
    "    # write bash script\n",
    "    with open('update_ids.sh', 'w') as f:\n",
    "        f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write('module load plink/1.9\\n')\n",
    "        f.write(f'plink --bfile {geno_path} --update-ids {ids_path} --make-bed --out {geno_path}_new_ids')\n",
    "        f.close()\n",
    "    \n",
    "    shell_do(f'bash update_ids.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51075e98-fd1f-4e86-ad56-c6e9b6ceb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_genotyping_phenos(covars_path, geno_path, out_path):\n",
    "    # read covars file \n",
    "    covars = pd.read_csv(covars_path, sep=',')\n",
    "    \n",
    "    # isolate MSBB to generate phenotype from coglev\n",
    "    msbb_covars = covars[covars['cohort'] == 'MSBB']\n",
    "    covars = covars[covars['cohort'] != 'MSBB']\n",
    "    msbb_covars['clinAD'] = np.where(msbb_covars['coglev'] == 'Dementia', 1, 0)\n",
    "    covars = msbb_covars.append(covars)\n",
    "    \n",
    "    # isolate ROSMAP to change IDs for MSBB and MayoRNA to str\n",
    "    rosmap_covars = covars[covars['cohort'] == 'ROSMAP']\n",
    "    covars = covars[covars['cohort'] != 'ROSMAP']\n",
    "    \n",
    "    # get rid of NA IDs\n",
    "    covars = covars[covars['wgs_id'].notna()]\n",
    "    \n",
    "    # change to str\n",
    "    covars['wgs_id'] = covars['wgs_id'].astype(float).astype(int).astype(str)\n",
    "    covars = covars.append(rosmap_covars)\n",
    "    \n",
    "    # isolate proper covars\n",
    "    covars_needed = covars.loc[:, ['wgs_id','female','clinAD']]\n",
    "    covars_needed.columns = ['IID','SEX','PHENO']\n",
    "    covars_needed['PHENO'] = covars_needed['PHENO'].astype(int)\n",
    "    \n",
    "    # read fam file\n",
    "    fam = read_fam(f'{geno_path}.fam')\n",
    "    fam = fam.drop(columns=['SEX','PHENO'], axis=1)\n",
    "    \n",
    "    # merge and switch sex and pheno to plink format\n",
    "    merge = fam.merge(covars_needed, how='inner', on=['IID'])\n",
    "    merge['SEX'] = np.where(merge['SEX'] == 1, 2, 1)\n",
    "    merge['PHENO'] = np.where(merge['PHENO'] == 1, 2, 1)\n",
    "    \n",
    "    # write IDs to txt\n",
    "    merge[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # only keep samples from intersection of covars and IDs\n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)\n",
    "    \n",
    "    # write to file\n",
    "    merge.to_csv(f'{out_path}.fam', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # new IDs\n",
    "    merge['FID_new'] = 'JG' + merge['FID'].astype(str)\n",
    "    merge['IID_new'] = 'JG' + merge['IID'].astype(str)\n",
    "    \n",
    "    # write new IDs to txt\n",
    "    merge[['FID','IID','FID_new','IID_new']].to_csv(f'{out_path}_new_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    update_ids(out_path, f'{out_path}_new_ids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe3338-d46f-4093-857a-01bfc10aa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_adni(geno_dir, out_path):\n",
    "    # getting out_dir\n",
    "    out_dir = os.path.dirname(out_path)\n",
    "    \n",
    "    # list of chromosome\n",
    "    chrom_list = [i for i in range(1,23)] + ['X']\n",
    "\n",
    "    # writing merge list 1 - original file location\n",
    "    with open('adni_merge_list1.txt', 'w') as f:\n",
    "        for chrom in chrom_list:\n",
    "            f.write(f'{geno_dir}/adni.nov2018.chr{chrom}\\n')\n",
    "        f.close()\n",
    "\n",
    "    # writing merge list 2 - project file location\n",
    "    with open('adni_merge_list2.txt', 'w') as f:\n",
    "        for chrom in chrom_list:\n",
    "            binary = f'adni.nov2018.chr{chrom}'\n",
    "            f.write(f'{out_dir}/{binary}\\n')\n",
    "        f.close()\n",
    "\n",
    "    # write bash script\n",
    "    with open('merge_adni.sh', 'w') as f:\n",
    "        f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write('module load plink/1.9\\n')\n",
    "\n",
    "        # first merge from original file location\n",
    "        merge_cmd = f'plink --merge-list adni_merge_list1.txt --allow-extra-chr --make-bed --out {out_path}'\n",
    "        f.write(f'{merge_cmd}\\n')\n",
    "\n",
    "        # excluding missnp variants from the individual chromosome files\n",
    "        for chrom in chrom_list:\n",
    "            binary = f'adni.nov2018.chr{chrom}'\n",
    "            exclude_cmd = f'plink --bfile {geno_dir}/{binary} --allow-extra-chr --exclude {out_path}-merge.missnp --make-bed --out {out_dir}/{binary}'\n",
    "            f.write(f'{exclude_cmd}\\n')\n",
    "\n",
    "        # second merge from project file location with exlcuded snps\n",
    "        merge_cmd2 = f'plink --merge-list adni_merge_list2.txt --allow-extra-chr --chr 1-23 --make-bed --out {out_path}'\n",
    "        f.write(f'{merge_cmd2}\\n')\n",
    "        \n",
    "        # removing individual chromosome files \n",
    "        f.write(f'rm {out_dir}/adni.nov2018.chr*')\n",
    "        f.close()\n",
    "\n",
    "    # write swarm script \n",
    "    with open('merge_adni.swarm', 'w') as f:\n",
    "        f.write('bash merge_adni.sh')\n",
    "        f.close()\n",
    "\n",
    "    # queue swarm job\n",
    "    swarm_cmd = 'swarm -f merge_adni.swarm -g 50 --time 12:00:00 --module plink/1.9'\n",
    "    shell_do(swarm_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46475aae-b8fd-4e96-8cd9-fafbdd5823ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adni_phenos(covars_path, geno_path, out_path):\n",
    "    # read covar file, rename needed columns\n",
    "    covars = pd.read_csv(covars_path, sep=',')\n",
    "    covars = covars.rename(columns={'PTID':'IID','DX_bl':'Diagnosis'})\n",
    "    \n",
    "    # isolate proper columns\n",
    "    covars_needed = covars.loc[:, ['IID','Diagnosis','AGE']]\n",
    "    \n",
    "    # read fam\n",
    "    fam = read_fam(f'{geno_path}.fam', True)\n",
    "    \n",
    "    # merge with covars and drop duplicates\n",
    "    merge = fam.merge(covars_needed, how='inner', on=['IID'])\n",
    "    merge = merge.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    # isolate CN and AD cases and change to plink format\n",
    "    merge = merge[(merge['Diagnosis']=='CN') | (merge['Diagnosis']=='AD')]\n",
    "    merge['PHENO'] = np.where(merge['Diagnosis'] == 'AD', 2, 1)\n",
    "    \n",
    "    # drop controls with age under 60\n",
    "    merge = merge.drop(merge[(merge['AGE'] < 60) & (merge['PHENO'] == 1)].index)\n",
    "    \n",
    "    # drop diagnosis and age columns\n",
    "    merge = merge.drop(columns=['Diagnosis','AGE'], axis=1)\n",
    "    \n",
    "    # write IDs to txt\n",
    "    merge[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # only keep samples from intersection of covars and IDs\n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)\n",
    "    \n",
    "    # write to file\n",
    "    merge.to_csv(f'{out_path}.fam', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daf9e7-b25c-4835-9638-df8bdf5a6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ftd_lbd_genos(geno_dir, ftd_out_dir, lbd_out_dir):\n",
    "    # write bash script to copy files into wd\n",
    "    with open('get_ftd_lbd.sh', 'w') as f:\n",
    "        f.write('#!/usr/bin/env bash\\n\\n')\n",
    "        f.write(f'cp {geno_dir}/merged_FTD.* {ftd_out_dir}\\n')\n",
    "        f.write(f'cp {geno_dir}/merged_LBD.* {lbd_out_dir}\\n')\n",
    "        f.close()\n",
    "    \n",
    "    # run bash script\n",
    "    shell_do(f'bash get_ftd_lbd.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75901d6b-d752-4315-b328-ef853cbff03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ftd_lbd_als(covars_path, ftd_geno_path, lbd_geno_path, ftd_out_path, lbd_out_path, als_out_path):\n",
    "    # read covars, isolate needed columns\n",
    "    covars = pd.read_csv(covars_path, sep='\\s+')\n",
    "    covars_needed = covars.loc[:, ['FID','CONSENSUS_AGE','PHENO']]\n",
    "    covars_needed.columns = ['FID','AGE','STATUS']\n",
    "    \n",
    "    # read FTD/ALS fam\n",
    "    ftd_als_fam = read_fam(f'{ftd_geno_path}.fam')\n",
    "    \n",
    "    # merge\n",
    "    ftd_als_merge = ftd_als_fam.merge(covars_needed, how='inner', on=['FID'])\n",
    "    \n",
    "    # drop controls with age under 60\n",
    "    ftd_als_merge = ftd_als_merge.drop(ftd_als_merge[(ftd_als_merge['AGE'] < 60) & (ftd_als_merge['PHENO'] == 1)].index)\n",
    "    \n",
    "    # isolate controls + FTD cases\n",
    "    ftd_merge = ftd_als_merge[(ftd_als_merge['STATUS'] == 'CONTROL') | (ftd_als_merge['STATUS'] == 'FTD')]\n",
    "    \n",
    "    # write IDs to txt\n",
    "    ftd_merge[['FID','IID']].to_csv(f'{ftd_out_path}_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # only keep IDs left\n",
    "    keep_cmd = f'plink2 --bfile {ftd_geno_path} --keep {ftd_out_path}_ids.txt --make-bed --out {ftd_out_path}'\n",
    "    shell_do(keep_cmd)\n",
    "    \n",
    "    # isolate controls + ALS cases\n",
    "    als_merge = ftd_als_merge[(ftd_als_merge['STATUS'] == 'CONTROL') | (ftd_als_merge['STATUS'] == 'ALS')]\n",
    "    \n",
    "    # write IDs to txt\n",
    "    als_merge[['FID','IID']].to_csv(f'{als_out_path}_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # only keep IDs left\n",
    "    keep_cmd = f'plink2 --bfile {ftd_geno_path} --keep {als_out_path}_ids.txt --make-bed --out {als_out_path}'\n",
    "    shell_do(keep_cmd)\n",
    "    \n",
    "    # read LBD fam\n",
    "    lbd_fam = read_fam(f'{lbd_geno_path}.fam')\n",
    "    \n",
    "    # merge\n",
    "    lbd_merge = lbd_fam.merge(covars_needed, how='inner', on=['FID'])\n",
    "    \n",
    "    # drop controls with age under 60\n",
    "    lbd_merge = lbd_merge.drop(lbd_merge[(lbd_merge['AGE'] < 60) & (lbd_merge['PHENO'] == 1)].index)\n",
    "    \n",
    "    # write IDs to txt\n",
    "    lbd_merge[['FID','IID']].to_csv(f'{lbd_out_path}_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # only keep IDs left\n",
    "    keep_cmd = f'plink2 --bfile {lbd_geno_path} --keep {lbd_out_path}_ids.txt --make-bed --out {lbd_out_path}'\n",
    "    shell_do(keep_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38cdc6-1822-4fad-b141-73d5ae3c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amp_pd_phenos(covars_path, geno_path, out_path):\n",
    "    # read covars, isolate needed columns\n",
    "    covars = pd.read_csv(covars_path, sep=',')\n",
    "    covars_needed = covars.loc[:,['FID','IID','SEX','PD_PHENO','AGE_ANALYSIS','DLB_PHENO']]\n",
    "    \n",
    "    # read fam file\n",
    "    fam = read_fam(f'{geno_path}.fam')\n",
    "    \n",
    "    # merge \n",
    "    merge = fam[['FID','IID','PAT','MAT']].merge(covars_needed, how='inner', on=['FID','IID'])\n",
    "    \n",
    "    # drop samples with LBD and NA PD pheno\n",
    "    merge = merge[(merge['DLB_PHENO'] != 2) & (merge['PD_PHENO'] != -9)]\n",
    "    \n",
    "    # drop controls with age under 60\n",
    "    merge = merge.drop(merge[(merge['AGE_ANALYSIS'] < 60) & (merge['PD_PHENO'] == 1)].index)\n",
    "    \n",
    "    # drop age and dlb columns\n",
    "    merge = merge.drop(columns=['AGE_ANALYSIS','DLB_PHENO'], axis=1)\n",
    "    \n",
    "    # write IDs to txt\n",
    "    merge[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    # only keep IDs left\n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)\n",
    "    \n",
    "    # write to file\n",
    "    merge.to_csv(f'{out_path}.fam', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7deb33-3bf6-4db8-b3d1-2f2d510803e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adsp_genos(pfiles, individual_paths, out_path):\n",
    "    # loop through individual chr pfiles and convert to bfiles\n",
    "    for i in range(22):\n",
    "        pfile = pfiles.replace('*', str(i+1))\n",
    "        individual_out = individual_paths.replace('*', str(i+1))\n",
    "        plink_cmd = f'plink2 --pfile {pfile} --make-bed --out {individual_out}'\n",
    "        shell_do(plink_cmd)\n",
    "    \n",
    "    # merge individual chr bfiles\n",
    "    merge_bfiles(individual_paths, adsp_geno_path, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ca866-7d9c-4b99-a98a-150c2bfca8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adsp_phenos(covars_path, geno_path, out_path):\n",
    "    # read covars, isolate needed columns\n",
    "    covars = pd.read_csv(covars_path, sep='\\t')\n",
    "    covars_needed = covars.loc[:,['SUBJID','Sex','Age','AD']]\n",
    "    \n",
    "    # read fam file\n",
    "    fam = read_fam(f'{geno_path}.fam', True)\n",
    "    \n",
    "    # build merge-ID for merging with covars\n",
    "    fam_id_split = fam['IID'].str.split(pat='-', expand=True)\n",
    "    fam_id_split['ID'] = fam_id_split[0] + '-' + fam_id_split[1] + '-' + fam_id_split[2]\n",
    "    fam['merge-ID'] = fam_id_split['ID']\n",
    "    \n",
    "    # merge covars and fam\n",
    "    merge = covars_needed.merge(fam, how='inner', left_on='SUBJID',right_on='merge-ID')\n",
    "    \n",
    "    # adjusting age and changing its type\n",
    "    merge['Age'] = np.where(merge['Age']=='90+', '90', merge['Age'])\n",
    "    merge['Age'] = merge['Age'].astype(float)\n",
    "    \n",
    "    # assigning sex and pheno\n",
    "    merge['SEX'] = np.where(merge['Sex'] == 0, 1, 2)\n",
    "    merge['PHENO'] = np.where(merge['AD'] == '0', 1, 2)\n",
    "    \n",
    "    # drop controls with age under 60\n",
    "    merge = merge.drop(merge[(merge['Age'] < 60) & (merge['AD'] == '0')].index)\n",
    "    \n",
    "    # drop covar columns \n",
    "    merge = merge.drop(columns=['SUBJID','Sex','Age','AD','merge-ID'], axis=1)\n",
    "    \n",
    "    # write IDs to txt\n",
    "    merge[['FID','IID']].to_csv(f'{out_path}_ids.txt', sep='\\t', header=False, index=False)\n",
    "    \n",
    "    # only keep IDs left\n",
    "    keep_cmd = f'plink2 --bfile {geno_path} --keep {out_path}_ids.txt --make-bed --out {out_path}'\n",
    "    shell_do(keep_cmd)\n",
    "    \n",
    "    # write to file\n",
    "    merge.to_csv(f'{out_path}.fam', sep='\\t', header=False, index=False)\n",
    "    \n",
    "    merge['FID_new'] = 'ADSP' + merge['IID'].astype(str)\n",
    "    merge['IID_new'] = 'ADSP' + merge['IID'].astype(str)\n",
    "          \n",
    "    # write new IDs to txt\n",
    "    merge[['FID','IID','FID_new','IID_new']].to_csv(f'{out_path}_new_ids.txt', sep='\\t', index=None, header=None)\n",
    "    \n",
    "    update_ids(out_path, f'{out_path}_new_ids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38540ea0-45d4-4e02-9401-98b6e55af7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project directory\n",
    "wd = 'insert_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dd021-1a9b-4673-8e36-67fd7981fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointGenotyping data paths\n",
    "jg_data_dir = 'insert_path'\n",
    "jg_seq_data_files = f'{jg_data_dir}/NIA_JG_1898_samples_GRM_WGS_b37_JointAnalysis01_2017-12-08_*.recalibrated_variants.vcf.gz'\n",
    "jg_out_vcf = f'{jg_data_dir}/temp.vcf'\n",
    "jg_out_file = f'{jg_data_dir}/all'\n",
    "\n",
    "vcf_to_pfile(jg_seq_data_files, jg_out_vcf, jg_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67235b72-e0f8-4e3e-be4f-546d0c812ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate jointGenotyping plink binaries\n",
    "jg_out_dir = f'{wd}/ROSMAPMayoRNAseqMSBB'\n",
    "jg_geno_path = f'{jg_out_dir}/joint_genotyping/jointGenotypingROSMAPMayoRNAseqMSBB'\n",
    "\n",
    "pfiles_to_bfiles(jg_out_file, jg_geno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d757d4-d8cb-4c7e-af3f-06d969eeec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get jointGenotyping phenos\n",
    "jg_covars_path = f'{jg_data_dir}/threeCohorts.csv'\n",
    "jg_out_path = f'{jg_geno_path}_pheno'\n",
    "\n",
    "get_joint_genotyping_phenos(jg_covars_path, jg_geno_path, jg_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c722e-5797-444b-9350-41fa7acdbf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_bim(f'{jg_out_path}_new_ids.bim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6e13a-e889-45d1-bc3d-48f085d3c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ADNI chromosomes\n",
    "adni_geno_dir = 'insert_path'\n",
    "adni_geno_path = f'{wd}/ADNI/genotypes/ADNI_all'\n",
    "\n",
    "merge_adni(adni_geno_dir, adni_geno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2fe92-ba8c-441e-8c16-9432fdc1738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ADNI phenos\n",
    "adni_covars_path = 'insert_path'\n",
    "adni_out_path = f'{wd}/ADNI/genotypes/ADNI_all_pheno'\n",
    "\n",
    "get_adni_phenos(adni_covars_path, adni_geno_path, adni_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafd1df-b725-47c9-b329-987fb430e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_bim(f'{adni_out_path}.bim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd3e99-5af6-4f54-b60b-91083647c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dementia seq genotypes\n",
    "ftd_lbd_geno_dir = 'insert_path'\n",
    "ftd_out_dir = f'{wd}/FTD_LBD_ALS/ftd_genotypes'\n",
    "lbd_out_dir = f'{wd}/FTD_LBD_ALS/lbd_genotypes'\n",
    "als_out_dir = f'{wd}/FTD_LBD_ALS/als_genotypes'\n",
    "\n",
    "get_ftd_lbd_genos(ftd_lbd_geno_dir, ftd_out_dir, lbd_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56d2c5-9a10-4893-9515-0025e2f445a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate ALS cases and get dementia seq phenos\n",
    "ftd_geno_path = f'{ftd_out_dir}/merged_FTD'\n",
    "lbd_geno_path = f'{lbd_out_dir}/merged_LBD'\n",
    "ftd_out_path = f'{ftd_out_dir}/merged_FTD_age_filter'\n",
    "lbd_out_path = f'{lbd_out_dir}/merged_LBD_age_filter'\n",
    "als_out_path = f'{als_out_dir}/merged_ALS_age_filter'\n",
    "ftd_lbd_covars_path = f'{ftd_lbd_geno_dir}/demseq_model1_covariates.txt'\n",
    "\n",
    "process_ftd_lbd_als(ftd_lbd_covars_path, ftd_geno_path, lbd_geno_path, ftd_out_path, lbd_out_path, als_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d06ac-2954-4bb2-bf6d-341d2ecf2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_bim(f'{ftd_out_path}.bim')\n",
    "reformat_bim(f'{lbd_out_path}.bim')\n",
    "reformat_bim(f'{als_out_path}.bim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd7638-89f7-44d2-a1bf-5b0700add38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get AMP-PD phenos\n",
    "pd_data_dir = 'insert_path'\n",
    "pd_covars_path = f'{pd_data_dir}/encoded_AMP_covs_SEPT2021.csv'\n",
    "pd_geno_path = f'{pd_data_dir}/amp_v2.5_formatted_split_normalized_allchr_plinkv19_maxAlleles2_EURO'\n",
    "pd_out_path = f'{wd}/AMP_PD/amp_pd_pheno'\n",
    "\n",
    "get_amp_pd_phenos(pd_covars_path, pd_geno_path, pd_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238bd6be-1b9d-4a43-bd91-b1e1b7526b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_bim(f'{pd_out_path}.bim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20387fc9-7e9e-44a7-84f3-9b6730beeb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ADSP genos\n",
    "adsp_data_dir = 'insert_path'\n",
    "adsp_pfiles = f'{adsp_data_dir}/chr*_formatted_normalized'\n",
    "adsp_out_dir = f'{wd}/ADSP'\n",
    "adsp_individual_out_path = f'{adsp_out_dir}/individual_chrs/chr*_formatted_normalized'\n",
    "adsp_geno_path = f'{adsp_out_dir}/adsp_formatted_normalized'\n",
    "\n",
    "get_adsp_genos(adsp_pfiles, adsp_individual_out_path, adsp_geno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0584ff-9370-4e9b-a9c4-e1499faa8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ADSP phenos\n",
    "adsp_covars_path = f'insert_path'\n",
    "adsp_out_path = f'{adsp_out_dir}/adsp_formatted_normalized_pheno'\n",
    "\n",
    "get_adsp_phenos(adsp_covars_path, adsp_geno_path, adsp_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c7060-5ffc-4f56-a567-3b1287527c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat_bim(f'{adsp_out_path}_new_ids.bim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f803e24-f3a4-42b9-bd46-2f223cf2b306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
